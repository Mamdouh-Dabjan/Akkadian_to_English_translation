{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Data cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97203ddc67e27b32"
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_english_text(text):\n",
    "    \"\"\"\n",
    "    Clean English text by removing unwanted patterns and formatting.\n",
    "    Transforming logic:\n",
    "    - Replace lit \"...\" with (literally, ...)\n",
    "    - Remove double quotes \"\"\n",
    "    - Remove asterisks * and other weird characters\n",
    "    - Keep back slashes \\\n",
    "    - Remove squared brackets []\n",
    "    - Remove normal brackets (), unless ( has a space before it and ) has a space after it\n",
    "    - Handle incomplete words with brackets (e.g., defe[ated -> defeated, Bū[du -> Būdu)\n",
    "    - Handles all kinds of quotes of any encoding\n",
    "    :param text: Input text string\n",
    "    :return: Cleaned text string\n",
    "    \"\"\"\n",
    "    # Handle NaN or None values\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to string if not already\n",
    "    text = str(text).strip()\n",
    "\n",
    "    # If empty after strip, return empty string\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Step 1: Handle orphaned closing parentheses first (before lit pattern)\n",
    "    # Pattern: word)-anything -> word-anything (for cases like Bīt)-Kapsi -> Bīt-Kapsi)\n",
    "    text = re.sub(r'(\\w+)\\)(-[^\\s]+)', r'\\1\\2', text)\n",
    "\n",
    "    # Step 2: Handle lit \"...\" pattern - transform to (literally, content)\n",
    "\n",
    "    # Step 1.5: Remove orphaned ')' inside words like 'pe)ple'\n",
    "    text = re.sub(r'(\\w+)\\)(\\w+)', r'\\1\\2', text)\n",
    "\n",
    "# Define comprehensive quote characters including all Unicode quote variants\n",
    "\n",
    "    # All possible quote characters (opening and closing)\n",
    "    quote_chars = [\n",
    "        # ASCII quotes\n",
    "        '\"', \"'\", '`',\n",
    "        # Unicode quotes - Left/Right Double Quotation Marks\n",
    "        '\\u201C', '\\u201D',  # \" \" (left/right double quotation marks)\n",
    "        '\\u201E', '\\u201F',  # „ ‟ (double low-9 quotation mark, double high-reversed-9 quotation mark)\n",
    "        # Unicode quotes - Left/Right Single Quotation Marks  \n",
    "        '\\u2018', '\\u2019',  # ' ' (left/right single quotation marks)\n",
    "        '\\u201A', '\\u201B',  # ‚ ‛ (single low-9 quotation mark, single high-reversed-9 quotation mark)\n",
    "        # Other Unicode quotes\n",
    "        '\\u00AB', '\\u00BB',  # « » (left/right-pointing double angle quotation marks)\n",
    "        '\\u2039', '\\u203A',  # ‹ › (single left/right-pointing angle quotation marks)\n",
    "        '\\u2E42', '\\u301D',  # ⹂ 〝 (double low-reversed-9 quotation mark, reversed double prime quotation mark)\n",
    "        '\\u301E', '\\u301F',  # 〞 〟 (double prime quotation mark, low double prime quotation mark)\n",
    "        # German quotes\n",
    "        '\\u201E', '\\u201C',  # „ \" (German style)\n",
    "        # French quotes  \n",
    "        '\\u00AB', '\\u00BB',  # « » (French style)\n",
    "        # Additional quote-like characters\n",
    "        '\\u02DD', '\\u02EE',  # ˝ ˮ (double acute accent, modifier letter double apostrophe)\n",
    "        # CJK quotes\n",
    "        '\\u300C', '\\u300D',  # 「 」 (left/right corner brackets)\n",
    "        '\\u300E', '\\u300F',  # 『 』 (left/right white corner brackets)\n",
    "    ]\n",
    "\n",
    "    # Create a character class for all quote characters\n",
    "    quote_class = '[' + ''.join(re.escape(char) for char in quote_chars) + ']'\n",
    "\n",
    "    # Enhanced lit pattern matching function\n",
    "    def lit_replacer(match):\n",
    "        content = match.group(1)\n",
    "        return f'(literally, {content})'\n",
    "\n",
    "    # Create comprehensive regex pattern for lit quotes\n",
    "    # This pattern looks for:\n",
    "    # - \"lit\" followed by whitespace\n",
    "    # - Any opening quote character\n",
    "    # - Content (non-greedy match of any characters except quote characters)\n",
    "    # - Any closing quote character\n",
    "    lit_pattern = rf'lit\\s+{quote_class}([^{quote_class[1:-1]}]*?){quote_class}'\n",
    "\n",
    "    # Apply the lit pattern transformation\n",
    "    text = re.sub(lit_pattern, lit_replacer, text)\n",
    "\n",
    "    # Fallback: Handle cases where quotes might be mixed or malformed\n",
    "    # This catches any remaining \"lit\" + quote patterns that might have been missed\n",
    "    fallback_patterns = [\n",
    "        # Handle specific problematic cases like chr(8220) and chr(8221)\n",
    "        r'lit\\s+[\\u2000-\\u206F\\u2E00-\\u2E7F\"\\'`\"\"''‚„‹›«»]([^\"\"''‚„‹›«»\"\\'`\\u2000-\\u206F\\u2E00-\\u2E7F]*?)[\\u2000-\\u206F\\u2E00-\\u2E7F\"\\'`\"\"''‚„‹›«»]',\n",
    "        # Ultra-broad fallback for any remaining quote-like characters\n",
    "        r'lit\\s+[\\u0020-\\u007E\\u00A0-\\u00FF\\u2000-\\u206F\\u2E00-\\u2E7F]*?([^a-zA-Z0-9\\s\\\\/-]*?)([^\"\"''‚„‹›«»\"\\'`]*?)[\\u0020-\\u007E\\u00A0-\\u00FF\\u2000-\\u206F\\u2E00-\\u2E7F]*?'\n",
    "    ]\n",
    "\n",
    "    for pattern in fallback_patterns:\n",
    "        if re.search(pattern, text):\n",
    "            text = re.sub(pattern, lit_replacer, text)\n",
    "            break\n",
    "\n",
    "    # Step 3: Handle incomplete words with square brackets\n",
    "    text = re.sub(r'(\\w+)\\[(\\w*)', r'\\1\\2', text)\n",
    "\n",
    "    # Step 4: Handle incomplete words with parentheses  \n",
    "    text = re.sub(r'(\\w+)\\((\\w*)', r'\\1\\2', text)\n",
    "\n",
    "    # Step 5: Remove remaining squared brackets and their content\n",
    "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
    "    text = re.sub(r'[\\[\\]]', '', text)\n",
    "\n",
    "    # Step 6: Handle normal brackets - keep only those with space before ( and space after )\n",
    "    valid_brackets = []\n",
    "    bracket_placeholder = \"|||VALID_BRACKET_{}|||\"\n",
    "\n",
    "    # Find all valid bracket patterns: space + ( + content + ) + space\n",
    "    valid_pattern = r'(?<=\\s)\\([^)]*\\)(?=\\s)'\n",
    "    matches = list(re.finditer(valid_pattern, text))\n",
    "\n",
    "    # Replace valid brackets with placeholders (in reverse order to maintain positions)\n",
    "    for i, match in enumerate(reversed(matches)):\n",
    "        placeholder = bracket_placeholder.format(len(matches) - 1 - i)\n",
    "        valid_brackets.insert(0, match.group())\n",
    "        text = text[:match.start()] + placeholder + text[match.end():]\n",
    "\n",
    "    # Now remove all remaining brackets\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "\n",
    "    # Restore valid brackets\n",
    "    for i, bracket_content in enumerate(valid_brackets):\n",
    "        placeholder = bracket_placeholder.format(i)\n",
    "        text = text.replace(placeholder, bracket_content)\n",
    "\n",
    "    # Step 7: Remove ALL types of quotes comprehensively\n",
    "    # Use the same comprehensive quote character class\n",
    "    text = re.sub(quote_class, '', text)\n",
    "\n",
    "    # Step 8: Remove asterisks and other weird characters (but keep backslashes)\n",
    "    weird_chars = r'[*#$%^&+=<>{}|~`¡¢£¤¥¦§¨©ª«¬®¯°±²³´µ¶·¸¹º»¼½¾¿×÷]'\n",
    "    text = re.sub(weird_chars, '', text)\n",
    "\n",
    "    # Step 9: Remove other potential weird Unicode characters but be conservative\n",
    "    text = re.sub(r'[†‡•…‰′″‴‵‶‷‸‹›※‼‽⁇⁈⁉⁏⁐⁑]', '', text)\n",
    "\n",
    "    # Step 10: Clean up multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    text = text.strip('\"')         # remove any \" at the very start/end\n",
    "    # Step 11: Final cleanup - remove leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Test the function with the provided examples and additional test cases\n",
    "test_cases = [\n",
    "    # New test cases based on the provided examples\n",
    "    'At that time I made a pointed iron \"arrow,\" in[scribed the mighty deeds of the god Aššur my lord on it and I set it up at the spring of the city Bīt-Ištar Upaš of the land Bīt)-Kapsi lit \"son of Kapsi\" assembled his people and ascended Mount Abirus I pursued him defe[ated him and carried off his booty',\n",
    "    'which are without num[ber the city Atu the tribe Qabiʾi 5 the fortress of Labbanat Arameans on the banks of River(s the tribe Bū[du',\n",
    "    \"In my ninth palû the god Aššur my lord encouraged me and I marched against the lands Bīt-Kapsi Bīt-Sangi Bīt-Urzakki Media lit \\\"land of the Medes\\\" Bīt-Zualzaš Bīt-Matti and Tupliyaš I captured plund[ered destroyed devastated and burned with fire the cities Bīt-Ištar Kinkangi Kindigiasu Kingialkasiš Kubušḫati[diš 5 Upušu Aḫsipuna Girgirâ and Kimbazḫati together with cities in their environs\",\n",
    "    # Test cases for various Unicode quotes\n",
    "    'lit \"standard quotes\"',\n",
    "    'lit \"unicode left/right quotes\"',\n",
    "    'lit 「CJK quotes」',\n",
    "    'lit «French quotes»',\n",
    "    'lit ‹single angle quotes›',\n",
    "    'pe)ple are testing',\n",
    "    f'lit {chr(8220)}chr 8220 quotes{chr(8221)}',  # Your specific case\n",
    "]\n",
    "\n",
    "print(\"Testing Enhanced English Text Cleaner:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, test_input in enumerate(test_cases, 1):\n",
    "    result = clean_english_text(test_input)\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"Input:  '{test_input}'\")\n",
    "    print(f\"Output: '{result}'\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Test specifically for the problematic cases\n",
    "print(\"\\nSpecific problem case tests:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "problem_cases = [\n",
    "    'The region of lit \"son of Kapsi\" was conquered',  # Should become The region of (literally, son of Kapsi) was conquered\n",
    "    f'lit {chr(8220)}son of Kapsi{chr(8221)}',  # Your specific chr(8220) case\n",
    "    'defe[ated',           # Should become defeated  \n",
    "    'River(s',             # Should become Rivers\n",
    "    'Bū[du',               # Should become Būdu\n",
    "    'num[ber',             # Should become number\n",
    "    'in[scribed',          # Should become inscribed\n",
    "    'Bīt)-Kapsi',          # Should become Bīt-Kapsi\n",
    "]\n",
    "\n",
    "for case in problem_cases:\n",
    "    result = clean_english_text(case)\n",
    "    print(f\"'{case}' -> '{result}'\")\n",
    "\n",
    "# Additional test for various quote combinations\n",
    "print(\"\\nComprehensive quote character tests:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "quote_test_cases = [\n",
    "    ('lit \"ASCII double\"', '(literally, ASCII double)'),\n",
    "    (\"lit 'ASCII single'\", '(literally, ASCII single)'),\n",
    "    ('lit \"Unicode left/right\"', '(literally, Unicode left/right)'),\n",
    "    ('lit ‚low-9 quotes‛', '(literally, low-9 quotes)'),\n",
    "    ('lit «angle quotes»', '(literally, angle quotes)'),\n",
    "    ('lit ‹single angles›', '(literally, single angles)'),\n",
    "    (f'lit {chr(8220)}chr 8220{chr(8221)}', '(literally, chr 8220)'),\n",
    "]\n",
    "\n",
    "for input_text, expected in quote_test_cases:\n",
    "    result = clean_english_text(input_text)\n",
    "    status = \"✓\" if expected in result else \"✗\"\n",
    "    print(f\"{status} '{input_text}' -> '{result}'\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-25T18:28:27.932745Z",
     "start_time": "2025-10-25T18:28:27.919911Z"
    }
   },
   "id": "2c9e27b226568439",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Enhanced English Text Cleaner:\n",
      "================================================================================\n",
      "Test 1:\n",
      "Input:  'At that time I made a pointed iron \"arrow,\" in[scribed the mighty deeds of the god Aššur my lord on it and I set it up at the spring of the city Bīt-Ištar Upaš of the land Bīt)-Kapsi lit \"son of Kapsi\" assembled his people and ascended Mount Abirus I pursued him defe[ated him and carried off his booty'\n",
      "Output: 'At that time I made a pointed iron arrow, inscribed the mighty deeds of the god Aššur my lord on it and I set it up at the spring of the city Bīt-Ištar Upaš of the land Bīt-Kapsi (literally, son of Kapsi) assembled his people and ascended Mount Abirus I pursued him defeated him and carried off his booty'\n",
      "--------------------------------------------------------------------------------\n",
      "Test 2:\n",
      "Input:  'which are without num[ber the city Atu the tribe Qabiʾi 5 the fortress of Labbanat Arameans on the banks of River(s the tribe Bū[du'\n",
      "Output: 'which are without number the city Atu the tribe Qabiʾi 5 the fortress of Labbanat Arameans on the banks of Rivers the tribe Būdu'\n",
      "--------------------------------------------------------------------------------\n",
      "Test 3:\n",
      "Input:  'In my ninth palû the god Aššur my lord encouraged me and I marched against the lands Bīt-Kapsi Bīt-Sangi Bīt-Urzakki Media lit \"land of the Medes\" Bīt-Zualzaš Bīt-Matti and Tupliyaš I captured plund[ered destroyed devastated and burned with fire the cities Bīt-Ištar Kinkangi Kindigiasu Kingialkasiš Kubušḫati[diš 5 Upušu Aḫsipuna Girgirâ and Kimbazḫati together with cities in their environs'\n",
      "Output: 'In my ninth palû the god Aššur my lord encouraged me and I marched against the lands Bīt-Kapsi Bīt-Sangi Bīt-Urzakki Media (literally, land of the Medes) Bīt-Zualzaš Bīt-Matti and Tupliyaš I captured plundered destroyed devastated and burned with fire the cities Bīt-Ištar Kinkangi Kindigiasu Kingialkasiš Kubušḫatidiš 5 Upušu Aḫsipuna Girgirâ and Kimbazḫati together with cities in their environs'\n",
      "--------------------------------------------------------------------------------\n",
      "Test 4:\n",
      "Input:  'lit \"standard quotes\"'\n",
      "Output: ''\n",
      "--------------------------------------------------------------------------------\n",
      "Test 5:\n",
      "Input:  'lit \"unicode left/right quotes\"'\n",
      "Output: ''\n",
      "--------------------------------------------------------------------------------\n",
      "Test 6:\n",
      "Input:  'lit 「CJK quotes」'\n",
      "Output: ''\n",
      "--------------------------------------------------------------------------------\n",
      "Test 7:\n",
      "Input:  'lit «French quotes»'\n",
      "Output: ''\n",
      "--------------------------------------------------------------------------------\n",
      "Test 8:\n",
      "Input:  'lit ‹single angle quotes›'\n",
      "Output: ''\n",
      "--------------------------------------------------------------------------------\n",
      "Test 9:\n",
      "Input:  'pe)ple are testing'\n",
      "Output: 'peple are testing'\n",
      "--------------------------------------------------------------------------------\n",
      "Test 10:\n",
      "Input:  'lit “chr 8220 quotes”'\n",
      "Output: ''\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Specific problem case tests:\n",
      "================================================================================\n",
      "'The region of lit \"son of Kapsi\" was conquered' -> 'The region of (literally, son of Kapsi) was conquered'\n",
      "'lit “son of Kapsi”' -> ''\n",
      "'defe[ated' -> 'defeated'\n",
      "'River(s' -> 'Rivers'\n",
      "'Bū[du' -> 'Būdu'\n",
      "'num[ber' -> 'number'\n",
      "'in[scribed' -> 'inscribed'\n",
      "'Bīt)-Kapsi' -> 'Bīt-Kapsi'\n",
      "\n",
      "Comprehensive quote character tests:\n",
      "========================================\n",
      "✗ 'lit \"ASCII double\"' -> ''\n",
      "✗ 'lit 'ASCII single'' -> ''\n",
      "✗ 'lit \"Unicode left/right\"' -> ''\n",
      "✗ 'lit ‚low-9 quotes‛' -> ''\n",
      "✗ 'lit «angle quotes»' -> ''\n",
      "✗ 'lit ‹single angles›' -> ''\n",
      "✗ 'lit “chr 8220”' -> ''\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "def process_csv_file(csv_filename):\n",
    "    \"\"\"\n",
    "    Process a CSV file by cleaning the 'english' column and showing before/after for each row.\n",
    "    :param csv_filename: Path to the CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_filename, encoding=\"utf-8-sig\")\n",
    "\n",
    "        # Check if 'english' column exists\n",
    "        if 'english' not in df.columns:\n",
    "            print(f\"Error: 'english' column not found in {csv_filename}\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "            return\n",
    "\n",
    "        print(f\"Processing {len(df)} rows from {csv_filename}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # Process each row\n",
    "        for index, row in df.iterrows():\n",
    "            original_text = row['english']\n",
    "            cleaned_text = clean_english_text(original_text)\n",
    "\n",
    "            print(f\"Row {index + 1}:\")\n",
    "            print(f\"BEFORE: {original_text}\")\n",
    "            print(\"-\" * 50)\n",
    "            print(f\"AFTER:  {cleaned_text}\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "            # Update the dataframe with cleaned text\n",
    "            df.at[index, 'english'] = cleaned_text\n",
    "\n",
    "        # Save the cleaned data to a new CSV file\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{csv_filename}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_file.csv' with the actual path to your CSV file\n",
    "    csv_file_path = \"data_directories/rinap/data_files/data_2.csv\"\n",
    "\n",
    "    # Uncomment the line below and replace with your actual CSV file path\n",
    "    process_csv_file(csv_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-20T14:50:51.098210Z",
     "start_time": "2025-07-20T14:50:51.087412Z"
    }
   },
   "id": "c84deee1ccf8a425",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 18 rows from data_directories/rinap/data_files/data_2.csv\n",
      "================================================================================\n",
      "Row 1:\n",
      "BEFORE: Precious scion of Baltil Aššur beloved of the god(dess DN and Šē]rūa creation of the goddess Ninmena who for the dominion of the lands who grew up to be king governor the one who increases voluntary offerings for of emblems 5 powerful male light of all of his people lord of all rulers the one who overwhelms his foes valiant man the one who destroys enemies who cuts straight through interlocking mountains like a taut string and\n",
      "--------------------------------------------------\n",
      "AFTER:  Precious scion of Baltil Aššur beloved of the goddess DN and Šērūa creation of the goddess Ninmena who for the dominion of the lands who grew up to be king governor the one who increases voluntary offerings for of emblems 5 powerful male light of all of his people lord of all rulers the one who overwhelms his foes valiant man the one who destroys enemies who cuts straight through interlocking mountains like a taut string and\n",
      "================================================================================\n",
      "Row 2:\n",
      "BEFORE: warrio[r who made bow down at his feet who pu[t to the sword lit “weapon” circumsp[ect\n",
      "--------------------------------------------------\n",
      "AFTER:  warrior who made bow down at his feet who put to the sword (literally, weapon) circumspect\n",
      "================================================================================\n",
      "Row 3:\n",
      "BEFORE: No translation possible\n",
      "--------------------------------------------------\n",
      "AFTER:  No translation possible\n",
      "================================================================================\n",
      "Row 4:\n",
      "BEFORE: he made kiss his feet mo]untains in/of battle he a god made my weapon/rule greater than all of those/the kings who sit on royal daises ci]rcumspect 20 exalted lio]n-dragon inhabited world\n",
      "--------------------------------------------------\n",
      "AFTER:  he made kiss his feet mountains in/of battle he a god made my weapon/rule greater than all of those/the kings who sit on royal daises circumspect 20 exalted lion-dragon inhabited world\n",
      "================================================================================\n",
      "Row 5:\n",
      "BEFORE: At the beginning of my reign in my first palû in the fifth month after I sat in greatness on the throne of kingship the god Aššur my lord encouraged me and I marched against the Aramean tribes Ḫamarānu Ḫamrānu Luḫuʾātu 25 Ḫatallu Rubbû Rapiqu Ḫīrānu Rabbi-ilu Naṣīru Gulūsu Nabātu Liʾtaʾu Raḫīqu Kapīri Rummulītu Rummulūtu Adilê Gibrê Ubūdu Gurūmu\n",
      "--------------------------------------------------\n",
      "AFTER:  At the beginning of my reign in my first palû in the fifth month after I sat in greatness on the throne of kingship the god Aššur my lord encouraged me and I marched against the Aramean tribes Ḫamarānu Ḫamrānu Luḫuʾātu 25 Ḫatallu Rubbû Rapiqu Ḫīrānu Rabbi-ilu Naṣīru Gulūsu Nabātu Liʾtaʾu Raḫīqu Kapīri Rummulītu Rummulūtu Adilê Gibrê Ubūdu Gurūmu\n",
      "================================================================================\n",
      "Row 6:\n",
      "BEFORE: he a eunuch of mine plun]dered those cities and brought that booty to the land Ḫatti Syria-Palestine before me I settled 600 captives of the city Amlāte of the tribe Damūnu and 5,400 captives of the city Bīt-Dērāya in the cities Kunalua Ḫuzarra Taʾe Tarmanazi Kulmadara Ḫatatirra and Irgillu cities of the land Unqi\n",
      "--------------------------------------------------\n",
      "AFTER:  he a eunuch of mine plundered those cities and brought that booty to the land Ḫatti Syria-Palestine before me I settled 600 captives of the city Amlāte of the tribe Damūnu and 5,400 captives of the city Bīt-Dērāya in the cities Kunalua Ḫuzarra Taʾe Tarmanazi Kulmadara Ḫatatirra and Irgillu cities of the land Unqi\n",
      "================================================================================\n",
      "Row 7:\n",
      "BEFORE: I settled captive highlanders lit “Gutians” of the land Bīt-Sangibūti 1,200 people of the tribe Illila and 6,208 people of the tribes Nakkaba and Būda 5 in the cities Ṣimirra Arqâ Usnû and Siʾannu cities on the seacoast\n",
      "--------------------------------------------------\n",
      "AFTER:  I settled captive highlanders (literally, Gutians) of the land Bīt-Sangibūti 1,200 people of the tribe Illila and 6,208 people of the tribes Nakkaba and Būda 5 in the cities Ṣimirra Arqâ Usnû and Siʾannu cities on the seacoast\n",
      "================================================================================\n",
      "Row 8:\n",
      "BEFORE: I settled 588 people of the tribes Būdu and Dunu 252 people of the tribe Bēlâ 554 people of the tribe Bānītu 380 people of the tribe Palil-andil-māti “The god Palil is the protecting shadow of the land” 460 people of the tribe Sangillu people of the tribe Illila 458 captive highlanders lit “Gutians” of the land Bīt-Sa]ngibūti in the province of the city Tuʾi[mmu\n",
      "--------------------------------------------------\n",
      "AFTER:  I settled 588 people of the tribes Būdu and Dunu 252 people of the tribe Bēlâ 554 people of the tribe Bānītu 380 people of the tribe Palil-andil-māti The god Palil is the protecting shadow of the land 460 people of the tribe Sangillu people of the tribe Illila 458 captive highlanders (literally, Gutians) of the land Bīt-Sangibūti in the province of the city Tuʾimmu\n",
      "================================================================================\n",
      "Row 9:\n",
      "BEFORE: I settled 555 captive highlanders lit “Gutians” of the city Bīt-Sangibū]ti in the city Tīl-karme I considered them as inhabitants of Assyria and imposed upon them corvée labor like that of the Assyrians\n",
      "--------------------------------------------------\n",
      "AFTER:  I settled 555 captive highlanders (literally, Gutians) of the city Bīt-Sangibūti in the city Tīl-karme I considered them as inhabitants of Assyria and imposed upon them corvée labor like that of the Assyrians\n",
      "================================================================================\n",
      "Row 10:\n",
      "BEFORE: The payment of Kuštašpi of the city Kummuḫu 10 Raḫiānu Rezin of the land Damascu[s Menahem of the city Samaria Hiram of the city Tyre Sibit]ti-Biʾil of the city Byblos Uriaikki of the land Qu[e Pisīris of the city Carchemish Ēnī-il of the city Hamath Pa]nammû of the city Samʾal Tarḫu-lara of the land Gurgu[m Sulumal of the land Melid Dadīlu of the city Kasku Uas]surme of the land Tabal Ušḫitti of the land A)tuna Urpallâ of the land Tuḫana Tuḫamme of the city Ištunda Urimmi of the city Ḫubišn]a Ḫubušnu and Zabibê queen of the Arabs gold silver tin iron elephant hides ivory multi-colored garments linen garments blue-purple and red-purple wool ebony boxwood 15 all kinds of precious things from the royal treasure live sheep whose wool is dyed red-purple flying birds of the sky whose wings are dyed blue-purple horses mules oxen and sheep and goats camels she-camels together with their young I recei[ved from them\n",
      "--------------------------------------------------\n",
      "AFTER:  The payment of Kuštašpi of the city Kummuḫu 10 Raḫiānu Rezin of the land Damascus Menahem of the city Samaria Hiram of the city Tyre Sibitti-Biʾil of the city Byblos Uriaikki of the land Que Pisīris of the city Carchemish Ēnī-il of the city Hamath Panammû of the city Samʾal Tarḫu-lara of the land Gurgum Sulumal of the land Melid Dadīlu of the city Kasku Uassurme of the land Tabal Ušḫitti of the land Atuna Urpallâ of the land Tuḫana Tuḫamme of the city Ištunda Urimmi of the city Ḫubišna Ḫubušnu and Zabibê queen of the Arabs gold silver tin iron elephant hides ivory multi-colored garments linen garments blue-purple and red-purple wool ebony boxwood 15 all kinds of precious things from the royal treasure live sheep whose wool is dyed red-purple flying birds of the sky whose wings are dyed blue-purple horses mules oxen and sheep and goats camels she-camels together with their young I received from them\n",
      "================================================================================\n",
      "Row 11:\n",
      "BEFORE: In my ninth palû the god Aššur my lord encouraged me and I marched against the lands Bīt-Kapsi Bīt-Sangi Bīt-Urzakki Media lit “land of the Medes” Bīt-Zualzaš Bīt-Matti and Tupliyaš I captured plund[ered destroyed devastated and burned with fire the cities Bīt-Ištar Kinkangi Kindigiasu Kingialkasiš Kubušḫati[diš 5 Upušu Aḫsipuna Girgirâ and Kimbazḫati together with cities in their environs\n",
      "--------------------------------------------------\n",
      "AFTER:  In my ninth palû the god Aššur my lord encouraged me and I marched against the lands Bīt-Kapsi Bīt-Sangi Bīt-Urzakki Media (literally, land of the Medes) Bīt-Zualzaš Bīt-Matti and Tupliyaš I captured plundered destroyed devastated and burned with fire the cities Bīt-Ištar Kinkangi Kindigiasu Kingialkasiš Kubušḫatidiš 5 Upušu Aḫsipuna Girgirâ and Kimbazḫati together with cities in their environs\n",
      "================================================================================\n",
      "Row 12:\n",
      "BEFORE: At that time I made a pointed iron “arrow,” in[scribed the mighty deeds of the god Aššur my lord on it and I set it up at the spring of the city Bīt-Ištar Upaš of the land Bīt)-Kapsi lit “son of Kapsi” assembled his people and ascended Mount Abirus I pursued him defe[ated him and carried off his booty\n",
      "--------------------------------------------------\n",
      "AFTER:  At that time I made a pointed iron arrow, inscribed the mighty deeds of the god Aššur my lord on it and I set it up at the spring of the city Bīt-Ištar Upaš of the land Bīt-Kapsi (literally, son of Kapsi) assembled his people and ascended Mount Abirus I pursued him defeated him and carried off his booty\n",
      "================================================================================\n",
      "Row 13:\n",
      "BEFORE: I captured and defeated the cities Ḫista Ḫarbisinna Barbaz and Tasa as far as the Uluruš River I carried off 8,650 people hors]es 300 mules 660 asses 1,350 oxen and 19,000 sheep Those cities I destroyed devastated and burned with fire I annexed their to Assyria rebuilt those cities and settled therein the people of foreign lands conquered by me and set up the weapon of the god Aššur my lord therein I added it the area to the province of the land Naʾiri\n",
      "--------------------------------------------------\n",
      "AFTER:  I captured and defeated the cities Ḫista Ḫarbisinna Barbaz and Tasa as far as the Uluruš River I carried off 8,650 people horses 300 mules 660 asses 1,350 oxen and 19,000 sheep Those cities I destroyed devastated and burned with fire I annexed their to Assyria rebuilt those cities and settled therein the people of foreign lands conquered by me and set up the weapon of the god Aššur my lord therein I added it the area to the province of the land Naʾiri\n",
      "================================================================================\n",
      "Row 14:\n",
      "BEFORE: I captured and de]feated the cities Daiqanša Sakka Ippa Elizanšu 5 Luqadanšu Quda Elugia Dania Danziun Ulāya Luqia Abrania Eusa I carried off 900 people 150 oxen 1,000 sheep horses mules and asses\n",
      "--------------------------------------------------\n",
      "AFTER:  I captured and defeated the cities Daiqanša Sakka Ippa Elizanšu 5 Luqadanšu Quda Elugia Dania Danziun Ulāya Luqia Abrania Eusa I carried off 900 people 150 oxen 1,000 sheep horses mules and asses\n",
      "================================================================================\n",
      "Row 15:\n",
      "BEFORE: I destroyed devastated and burned with fire the]ir cities The people of the land Muqania saw the dust cloud of my expeditionary force and the city of Ura which is in midst of the land Muṣurni their sons their daughters their fam ily 10 I cut off their hands and I released them in their own land I carried off horses mules I destroyed devastated and burned with fire I captured destroyed devastated and burned with fire I captured and defeated him\n",
      "--------------------------------------------------\n",
      "AFTER:  I destroyed devastated and burned with fire their cities The people of the land Muqania saw the dust cloud of my expeditionary force and the city of Ura which is in midst of the land Muṣurni their sons their daughters their fam ily 10 I cut off their hands and I released them in their own land I carried off horses mules I destroyed devastated and burned with fire I captured destroyed devastated and burned with fire I captured and defeated him\n",
      "================================================================================\n",
      "Row 16:\n",
      "BEFORE: which are without num[ber the city Atu the tribe Qabiʾi 5 the fortress of Labbanat Arameans on the banks of River(s the tribe Bū[du\n",
      "--------------------------------------------------\n",
      "AFTER:  which are without number the city Atu the tribe Qabiʾi 5 the fortress of Labbanat Arameans on the banks of Rivers the tribe Būdu\n",
      "================================================================================\n",
      "Row 17:\n",
      "BEFORE: I crossed o]n rafts I all of the Arameans I carried off thousand and 9,000 people thousand and 500 oxen 10 I destroyed devastated and burned with fire The terrifying radiance of the god Aššur my lord overwhelmed the chief[tains of the Chaldeans and they They the chieftains came before me and kissed my feet\n",
      "--------------------------------------------------\n",
      "AFTER:  I crossed on rafts I all of the Arameans I carried off thousand and 9,000 people thousand and 500 oxen 10 I destroyed devastated and burned with fire The terrifying radiance of the god Aššur my lord overwhelmed the chieftains of the Chaldeans and they They the chieftains came before me and kissed my feet\n",
      "================================================================================\n",
      "Row 18:\n",
      "BEFORE: the temple personnel of Esagil Ezida and E[meslam brought before me the sacrificial remnants of the gods Bēl Marduk Nabû and Nergal\n",
      "--------------------------------------------------\n",
      "AFTER:  the temple personnel of Esagil Ezida and Emeslam brought before me the sacrificial remnants of the gods Bēl Marduk Nabû and Nergal\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "def clean_visible_text_akkadian(text):\n",
    "    \"\"\"\n",
    "    Clean visible text by removing unwanted patterns and formatting.\n",
    "    Transforming logic:\n",
    "    any dots in the end will be replaced with hyphens\n",
    "    any hyphens in the beginning or end will be removed\n",
    "    any special characters will be removed\n",
    "    any logograms associated with missing values like x or [...] will be omitted all together\n",
    "    any brackets will be removed\n",
    "    any standalone x will be removed\n",
    "    any standalone x between dots will be removed\n",
    "    numbers will be kept as is, if they are surrounded by dots, the dots are replaced by hyphens\n",
    "    :param text: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    # Start with basic strip\n",
    "    text = text.strip()\n",
    "\n",
    "    # If empty after strip, return empty string\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # ✅ NEW: Remove letter followed by brackets pattern (robust approach)\n",
    "    # This handles cases like \"d[complex content with nested brackets]\"\n",
    "    # Use a more aggressive approach since brackets may not be balanced\n",
    "    if re.match(r'^[a-zA-Z]\\[', text):\n",
    "        # If text starts with letter followed by bracket, remove everything\n",
    "        text = \"\"\n",
    "    else:\n",
    "        # Otherwise, remove any letter(s)-bracket patterns we can find\n",
    "        # Keep applying until no more changes\n",
    "        prev_text = \"\"\n",
    "        while prev_text != text:\n",
    "            prev_text = text\n",
    "            # FIXED: Match multiple letters followed by optional hyphen and brackets\n",
    "            text = re.sub(r'[a-zA-Z]+-?\\[(?:[^\\[\\]]|\\[[^\\]]*\\])*\\]', '', text)\n",
    "            text = re.sub(r'[a-zA-Z]+-?\\[[^\\]]*\\]', '', text)\n",
    "            # Also catch cases where there might be punctuation before the letters\n",
    "            text = re.sub(r'[.,;:-]*[a-zA-Z]+\\[(?:[^\\[\\]]|\\[[^\\]]*\\])*\\]', '', text)\n",
    "            text = re.sub(r'[.,;:-]*[a-zA-Z]+\\[[^\\]]*\\]', '', text)\n",
    "        text = text.strip()\n",
    "\n",
    "    # If we cleared everything, return empty\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # ✅ NEW: Remove patterns like \"something.[...]\" including bracketed words like \"⸢KUR⸣.[...]\"\n",
    "    text = re.sub(r'[⸢⸤]?[\\w⸢⸣⸤⸥]+[⸣⸥]?\\s*\\.\\[[^\\]]*\\]', '', text).strip()\n",
    "\n",
    "    # ✅ NEW: Remove patterns like \"something.x\" including bracketed words like \"⸢KUR⸣.x\"\n",
    "    text = re.sub(r'[⸢⸤]?[\\w⸢⸣⸤⸥]+[⸣⸥]?\\s*\\.[xX]', '', text).strip()\n",
    "\n",
    "    # ✅ Step 1: Remove leading garbage like \"...]-\" or \"[x]-\"\n",
    "    text = re.sub(r\"^.*[\\[\\](){}<>]+.*\\]-+\", \"\", text).strip()\n",
    "\n",
    "    # ✅ Step 2: Strip trailing \"-[...]\" or similar\n",
    "    text = re.sub(r\"-+[\\[\\](){}<>].*$\", \"\", text).strip()\n",
    "\n",
    "    # ✅ Step 3: Remove bracketed content entirely\n",
    "    # text = re.sub(r\"[\\[\\](){}<>][^[\\](){}<>]*[\\]\\})>]\", \"\", text).strip()\n",
    "\n",
    "    # ✅ Step 4: Remove any remaining standalone brackets\n",
    "    text = re.sub(r\"[\\[\\](){}<>]\", \"\", text).strip()\n",
    "\n",
    "    # ✅ Step 5: Remove pure garbage patterns (only x, punctuation, spaces)\n",
    "    text = re.sub(r\"^[xX.\\- ]+$\", \"\", text).strip()\n",
    "\n",
    "    # ✅ Step 6: Replace dots with hyphens around numbers (e.g., .123. -> -123-)\n",
    "    text = re.sub(r\"\\.(\\d+)\\.\", r\"-\\1-\", text)\n",
    "\n",
    "    text = re.sub(r\"^x-|-x$\", \"\", text)\n",
    "\n",
    "    # ✅ Step 7: Remove hyphens (but keep hyphens between numbers)\n",
    "    text = re.sub(r\"(?<!\\d)-(?!\\d)\", \"\", text)\n",
    "\n",
    "    # ✅ Step 8: Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # ✅ Step 9: Remove weird characters like asterisks\n",
    "    text = re.sub(r\"[*]\", \"\", text)\n",
    "\n",
    "    # ✅ Step 10: Remove x alone or x between dots\n",
    "    text = re.sub(r\"\\b[x]\\b|\\.+[x]\\.+\", \"\", text)\n",
    "\n",
    "    # ✅ Step 11: Clean up multiple spaces and dots\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Multiple spaces to single space\n",
    "    text = re.sub(r\"\\.{3,}\", \"...\", text)  # Multiple dots to max 3\n",
    "\n",
    "    # ✅ Step 11.5: Remove specific unwanted characters\n",
    "    text = re.sub(\"ʾ\", \"\", text)  # Remove specific unwanted character\n",
    "\n",
    "    # ✅ Step 12: Remove leading/trailing punctuation except periods\n",
    "    text = re.sub(r\"^[^\\w.]+|[^\\w.]+$\", \"\", text)\n",
    "\n",
    "    # ✅ Step 13: Final cleanup - if only punctuation/whitespace remains, clear it\n",
    "    if not re.search(r\"[a-zA-Z0-9]\", text):\n",
    "        text = \"\"\n",
    "    # ✅ Step 14: swap the remaining periods with hyphens\n",
    "    text = re.sub(r\"\\.\", \"-\", text)\n",
    "\n",
    "    # ✅ Step 15: Remove any remaining individual unwanted characters (cleanup)\n",
    "    text = re.sub(r\"[⸢⸣⸤⸥]\", \"\", text)  # Remove any leftover weird bracket characters\n",
    "\n",
    "    # ✅ Step 16: Remove any trailing or leading hyphens\n",
    "    text = re.sub(r\"^-|-$\", \"\", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Test the function with multiple examples\n",
    "test_cases = [\n",
    "    \"[...].,.[...].,.[...].,.a-[...]\",\n",
    "    \"in-[...]\",\n",
    "    \"the-[missing text]\",\n",
    "    \"word-[complex content]\",\n",
    "    \"abc-[...]\"\n",
    "]\n",
    "\n",
    "for test_input in test_cases:\n",
    "    result = clean_visible_text_akkadian(test_input)\n",
    "    print(f\"Input: {test_input}\")\n",
    "    print(f\"Output: '{result}'\")\n",
    "    print()\n",
    "\n",
    "\n",
    "lines = open(\"data_directories/suhu/unlinked_files/unlinked_data_page_1.txt\", \"r\", encoding=\"utf-8-sig\").readlines()\n",
    "for i, line in enumerate(lines, 1):\n",
    "    original = line.strip()\n",
    "    cleaned = clean_visible_text_akkadian(original)\n",
    "\n",
    "\n",
    "    print(f\"\\nLine {i}:\")\n",
    "    print(f\"  BEFORE: '{original}'\")\n",
    "    print(f\"  AFTER:  '{cleaned}'\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-25T18:45:53.768774Z",
     "start_time": "2025-10-25T18:45:53.760260Z"
    }
   },
   "id": "35fe799f108f9a98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [...].,.[...].,.[...].,.a-[...]\n",
      "Output: ''\n",
      "\n",
      "Input: in-[...]\n",
      "Output: ''\n",
      "\n",
      "Input: the-[missing text]\n",
      "Output: ''\n",
      "\n",
      "Input: word-[complex content]\n",
      "Output: ''\n",
      "\n",
      "Input: abc-[...]\n",
      "Output: ''\n",
      "\n",
      "\n",
      "Line 1:\n",
      "  BEFORE: '3: [(...)]'\n",
      "  AFTER:  '3: --'\n",
      "\n",
      "Line 2:\n",
      "  BEFORE: '4b: (...)]'\n",
      "  AFTER:  '4b: --'\n",
      "\n",
      "Line 3:\n",
      "  BEFORE: '10b: ⸢4⸣.,.12.,.20.,.4'\n",
      "  AFTER:  '10b: 4-,-12-,-20-,-4'\n",
      "\n",
      "Line 4:\n",
      "  BEFORE: '13: 45.,.9'\n",
      "  AFTER:  '13: 45-,-9'\n",
      "\n",
      "Line 5:\n",
      "  BEFORE: '19b: [2'\n",
      "  AFTER:  '19b: 2'\n",
      "\n",
      "Line 6:\n",
      "  BEFORE: '22: 1'\n",
      "  AFTER:  '22: 1'\n",
      "\n",
      "Line 7:\n",
      "  BEFORE: '24b: 4.,.2'\n",
      "  AFTER:  '24b: 4-,-2'\n",
      "\n",
      "Line 8:\n",
      "  BEFORE: '26: [x.,.x.,.x.,.x.,.x.,.x]'\n",
      "  AFTER:  '26: -,,,,,'\n",
      "\n",
      "Line 9:\n",
      "  BEFORE: '29: [x.,.x.,.x.,.x.,.x.,.(x.,.x)].,.x.,.x.,.x.,.x.,.(x.,.x)]'\n",
      "  AFTER:  '29: -,,,,,,,,,,,,'\n",
      "\n",
      "Line 10:\n",
      "  BEFORE: '32: 1.,.5.,.2.,.20.,.3'\n",
      "  AFTER:  '32: 1-,-5-,-2-,-20-,-3'\n",
      "\n",
      "Line 11:\n",
      "  BEFORE: '34: 2'\n",
      "  AFTER:  '34: 2'\n",
      "\n",
      "Line 12:\n",
      "  BEFORE: '35b: 1.,.7'\n",
      "  AFTER:  '35b: 1-,-7'\n",
      "\n",
      "Line 13:\n",
      "  BEFORE: '36: 80'\n",
      "  AFTER:  '36: 80'\n",
      "\n",
      "Line 14:\n",
      "  BEFORE: '39b: 2'\n",
      "  AFTER:  '39b: 2'\n",
      "\n",
      "Line 15:\n",
      "  BEFORE: '41b: 10-šú'\n",
      "  AFTER:  '41b: 10-šú'\n",
      "\n",
      "Line 16:\n",
      "  BEFORE: '43b: 1-et'\n",
      "  AFTER:  '43b: 1-et'\n",
      "\n",
      "Line 17:\n",
      "  BEFORE: '46b: [x.,.x.,.x.,.x]'\n",
      "  AFTER:  '46b: -,,,'\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_and_clean_unlinked_data(text_content):\n",
    "    lines = text_content.strip().split('\\n')\n",
    "    temp = [line.strip().split(\":\", 1) for line in lines if line.strip()]\n",
    "    temp_2 = [(line[0], line[1].strip()) for line in temp if len(line) == 2]\n",
    "    final_result = []\n",
    "    for line_num, content in temp_2:\n",
    "        parts = content.split(\".,.\")\n",
    "        cleaned_parts = [clean_visible_text_akkadian(part) for part in parts]\n",
    "        final_result.extend(cleaned_parts)\n",
    "    return final_result\n",
    "\n",
    "def replace_data_markers(akkadian_text, cleaned_data, data_index):\n",
    "    result_text = akkadian_text\n",
    "    current_index = data_index\n",
    "    while '<data>' in result_text and current_index < len(cleaned_data):\n",
    "        replacement_text = cleaned_data[current_index].strip()\n",
    "        if replacement_text:\n",
    "            result_text = result_text.replace('<data>', replacement_text, 1)\n",
    "        else:\n",
    "            if '<data> ' in result_text:\n",
    "                result_text = result_text.replace('<data> ', '', 1)\n",
    "            else:\n",
    "                result_text = result_text.replace('<data>', '', 1)\n",
    "        current_index += 1\n",
    "    return result_text.strip(), current_index\n",
    "\n",
    "def process_files():\n",
    "    orig_path = \"data_directories/saao/\"\n",
    "    data_dir = Path(orig_path + 'data_files')\n",
    "    unlinked_dir = Path(orig_path + 'unlinked_files')\n",
    "    processed_dir = Path(orig_path + 'processed')\n",
    "    processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    if not data_dir.exists():\n",
    "        print(f\"Error: {data_dir} directory not found!\")\n",
    "        return\n",
    "    if not unlinked_dir.exists():\n",
    "        print(f\"Error: {unlinked_dir} directory not found!\")\n",
    "        return\n",
    "\n",
    "    csv_files = list(data_dir.glob('*.csv'))\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {data_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"\\nProcessing: {csv_file.name}\")\n",
    "        file_stem = csv_file.stem\n",
    "        unlinked_file = unlinked_dir / f\"unlinked_data_page_{file_stem}.txt\"\n",
    "\n",
    "        if not unlinked_file.exists():\n",
    "            possible_patterns = [\n",
    "                f\"unlinked_data_page_{file_stem.split('_')[-1]}.txt\" if '_' in file_stem else None,\n",
    "                f\"unlinked_data_{file_stem}.txt\",\n",
    "                f\"unlinked_{file_stem}.txt\"\n",
    "            ]\n",
    "            for pattern in possible_patterns:\n",
    "                if pattern and (unlinked_dir / pattern).exists():\n",
    "                    unlinked_file = unlinked_dir / pattern\n",
    "                    break\n",
    "\n",
    "        try:\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig', newline='') as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                rows = list(reader)\n",
    "                fieldnames = reader.fieldnames\n",
    "\n",
    "            if 'akkadian' not in fieldnames:\n",
    "                print(f\"Warning: 'akkadian' column not found in {csv_file.name}\")\n",
    "                output_file = processed_dir / f\"data_file_processed_{file_stem}.csv\"\n",
    "                with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                    writer.writeheader()\n",
    "                    writer.writerows(rows)\n",
    "                continue\n",
    "\n",
    "            data_index = 0\n",
    "            cleaned_data = []\n",
    "\n",
    "            if unlinked_file.exists():\n",
    "                print(f\"Found corresponding unlinked file: {unlinked_file.name}\")\n",
    "                with open(unlinked_file, 'r', encoding='utf-8-sig') as f:\n",
    "                    unlinked_content = f.read()\n",
    "                cleaned_data = parse_and_clean_unlinked_data(unlinked_content)\n",
    "                print(f\"Extracted {len(cleaned_data)} data items from unlinked file\")\n",
    "            else:\n",
    "                print(f\"No corresponding unlinked file found for {csv_file.name}\")\n",
    "                print(\"Applying English transformation only...\")\n",
    "\n",
    "            # ✅ Process all rows (always clean English; conditionally replace Akkadian <data>)\n",
    "            for row in rows:\n",
    "                if cleaned_data and '<data>' in row['akkadian']:\n",
    "                    original_akkadian = row['akkadian']\n",
    "                    count_before = original_akkadian.count('<data>')\n",
    "                    start_idx = data_index\n",
    "                    row['akkadian'], data_index = replace_data_markers(\n",
    "                        row['akkadian'], cleaned_data, data_index\n",
    "                    )\n",
    "                    print(f\"Replaced {data_index - start_idx} <data> in Akkadian (originally {count_before})\")\n",
    "\n",
    "                if 'english' in row:\n",
    "                    original_english = row['english']\n",
    "                    row['english'] = clean_english_text(row['english'])\n",
    "                    if original_english != row['english']:\n",
    "                        print(f\"Old English {original_english}'\")\n",
    "                        print(f\"Cleaned English '{row['english']}'\")\n",
    "                else:\n",
    "                    print(\"⚠️  'english' column not found in row — skipping English cleaning.\")\n",
    "\n",
    "\n",
    "# ✅ Save processed CSV\n",
    "            output_file = processed_dir / f\"data_file_processed_{file_stem}.csv\"\n",
    "            with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(rows)\n",
    "\n",
    "            print(f\"Saved processed file: {output_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {csv_file.name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nProcessing complete! Check the '{processed_dir}' directory for results.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_files()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e91aa62ca78aae6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combine all CSVs in the processed directories"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3a9d717797e3336"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def combine_csvs_in_directory(directory):\n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"❌ Directory does not exist: {directory}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "    if not all_files:\n",
    "        print(f\"⚠️  No CSV files found in {directory}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    dataframes = []\n",
    "\n",
    "    for file in all_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    if dataframes:\n",
    "        combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "        print(f\"✅ Combined {len(dataframes)} files into DataFrame with {len(combined_df)} rows\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Updated paths - add \"Dr. Azar Research/\" prefix\n",
    "data_dir = \"data_directories/\"\n",
    "end_dir = \"/processed\"\n",
    "\n",
    "    # Now your original code should work\n",
    "combined_df_suhu = combine_csvs_in_directory(data_dir + 'suhu' + end_dir)\n",
    "combined_df_saao = combine_csvs_in_directory(data_dir + 'saao' + end_dir)\n",
    "combined_df_rinap = combine_csvs_in_directory(data_dir + 'rinap' + end_dir)\n",
    "combined_df_ribo = combine_csvs_in_directory(data_dir + 'ribo' + end_dir)\n",
    "combined_df_raio = combine_csvs_in_directory(data_dir + 'raio' + end_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-25T20:08:44.350642Z",
     "start_time": "2025-10-25T20:07:00.288737Z"
    }
   },
   "id": "acc43c26d9a7af4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined 27 files into DataFrame with 260 rows\n",
      "✅ Combined 4976 files into DataFrame with 43479 rows\n",
      "✅ Combined 989 files into DataFrame with 6137 rows\n",
      "✅ Combined 398 files into DataFrame with 2639 rows\n",
      "✅ Combined 1702 files into DataFrame with 8416 rows\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "combined_df_ribo.to_csv(data_dir + \"ribo/combined_ribo_processed.csv\", index=False, encoding=\"utf-8\")\n",
    "combined_df_suhu.to_csv(data_dir + \"suhu/combined_suhu_processed.csv\", index=False, encoding=\"utf-8\")\n",
    "combined_df_saao.to_csv(data_dir + \"saao/combined_saao_processed.csv\", index=False, encoding=\"utf-8\")\n",
    "combined_df_rinap.to_csv(data_dir + \"rinap/combined_rinap_processed.csv\", index=False, encoding=\"utf-8\")\n",
    "combined_df_raio.to_csv(data_dir + \"raio/combined_raio_processed.csv\", index=False, encoding=\"utf-8\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-25T20:08:44.707010Z",
     "start_time": "2025-10-25T20:08:44.389775Z"
    }
   },
   "id": "fa20424ce79a698b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "df_combined = pd.concat([combined_df_suhu,\n",
    "                         combined_df_saao,\n",
    "                         combined_df_rinap,\n",
    "                         combined_df_raio,\n",
    "                         combined_df_ribo],\n",
    "                        ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-25T20:08:44.720980Z",
     "start_time": "2025-10-25T20:08:44.714028Z"
    }
   },
   "id": "6c35a61bee4ac3ab",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final checkup on the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb8486d79eef06b7"
  },
  {
   "cell_type": "code",
   "source": [
    "df_combined_working = df_combined.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-25T20:30:57.151933Z",
     "start_time": "2025-10-25T20:30:57.136973Z"
    }
   },
   "id": "ca5f0015577321d9",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:30:57.357601Z",
     "start_time": "2025-10-25T20:30:57.302203Z"
    }
   },
   "cell_type": "code",
   "source": "df_combined_working = df_combined_working[~df_combined_working['english'].str.contains(\"translation\", case=False, na=False)]",
   "id": "6010a1d8acb29544",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:30:57.448501Z",
     "start_time": "2025-10-25T20:30:57.442064Z"
    }
   },
   "cell_type": "code",
   "source": "df_combined_working[df_combined_working['akkadian'].isna()]",
   "id": "a47eca11756842e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          line akkadian                                            english  \\\n",
       "41        i 2'      NaN                                                his   \n",
       "182       r 4'      NaN                                        I/he caused   \n",
       "222       i 1'      NaN  I Ninurta-kudurrī-uṣur governor of the land of...   \n",
       "1588       r 3      NaN                                           The fine   \n",
       "1800   e. ii 1      NaN                     Aramaic caption Dayyan-Kurbail   \n",
       "...        ...      ...                                                ...   \n",
       "59016       1'      NaN                                               am I   \n",
       "59193     i 11      NaN                                      the fifth day   \n",
       "59292      i 6      NaN                    king of justice king of Babylon   \n",
       "59310     iv 8      NaN                                     Nebuchadnezzar   \n",
       "59341    o 15'      NaN                                                his   \n",
       "\n",
       "                                                  source  \n",
       "41     https://oracc.museum.upenn.edu/suhu/Q006206?la...  \n",
       "182    https://oracc.museum.upenn.edu/suhu/Q006210?la...  \n",
       "222    https://oracc.museum.upenn.edu/suhu/Q006214?la...  \n",
       "1588   https://oracc.museum.upenn.edu/saao/P336188?la...  \n",
       "1800   https://oracc.museum.upenn.edu/saao/P335279?la...  \n",
       "...                                                  ...  \n",
       "59016  https://oracc.museum.upenn.edu/ribo/Q005446?la...  \n",
       "59193  https://oracc.museum.upenn.edu/ribo/Q006302?la...  \n",
       "59292  https://oracc.museum.upenn.edu/ribo/Q006241?la...  \n",
       "59310  https://oracc.museum.upenn.edu/ribo/Q006241?la...  \n",
       "59341  https://oracc.museum.upenn.edu/ribo/Q006246?la...  \n",
       "\n",
       "[172 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line</th>\n",
       "      <th>akkadian</th>\n",
       "      <th>english</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>i 2'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>his</td>\n",
       "      <td>https://oracc.museum.upenn.edu/suhu/Q006206?la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>r 4'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I/he caused</td>\n",
       "      <td>https://oracc.museum.upenn.edu/suhu/Q006210?la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>i 1'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I Ninurta-kudurrī-uṣur governor of the land of...</td>\n",
       "      <td>https://oracc.museum.upenn.edu/suhu/Q006214?la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>r 3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The fine</td>\n",
       "      <td>https://oracc.museum.upenn.edu/saao/P336188?la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>e. ii 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aramaic caption Dayyan-Kurbail</td>\n",
       "      <td>https://oracc.museum.upenn.edu/saao/P335279?la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59016</th>\n",
       "      <td>1'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>am I</td>\n",
       "      <td>https://oracc.museum.upenn.edu/ribo/Q005446?la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59193</th>\n",
       "      <td>i 11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the fifth day</td>\n",
       "      <td>https://oracc.museum.upenn.edu/ribo/Q006302?la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59292</th>\n",
       "      <td>i 6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>king of justice king of Babylon</td>\n",
       "      <td>https://oracc.museum.upenn.edu/ribo/Q006241?la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59310</th>\n",
       "      <td>iv 8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nebuchadnezzar</td>\n",
       "      <td>https://oracc.museum.upenn.edu/ribo/Q006241?la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59341</th>\n",
       "      <td>o 15'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>his</td>\n",
       "      <td>https://oracc.museum.upenn.edu/ribo/Q006246?la...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>172 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "source": [
    "df_combined_working.duplicated().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-25T20:31:00.123203Z",
     "start_time": "2025-10-25T20:31:00.095541Z"
    }
   },
   "id": "6b3b40490680cff9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(6)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "source": [
    "# Solving the (next b) problem\n",
    "import pandas as pd\n",
    "\n",
    "def merge_consecutive_b_rows_anchor(df):\n",
    "    \"\"\"\n",
    "    Merge consecutive 'b' lines from the same source, keeping the first as anchor.\n",
    "    Returns a cleaned DataFrame with merged rows.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "    \n",
    "    # Work with a copy to avoid modifying the original\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        # Skip if current row doesn't contain 'b' or line is not a string\n",
    "        if not isinstance(df.at[i, 'line'], str) or 'b' not in df.at[i, 'line']:\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        # This row contains 'b', use it as anchor\n",
    "        anchor_source = df.at[i, 'source']\n",
    "        merged_lines = [df.at[i, 'line']]\n",
    "        rows_to_merge = []\n",
    "        \n",
    "        # Look ahead for consecutive 'b' rows from the same source\n",
    "        j = i + 1\n",
    "        while j < len(df):\n",
    "            # Check if this row should be merged\n",
    "            if (isinstance(df.at[j, 'line'], str) and \n",
    "                'b' in df.at[j, 'line'] and \n",
    "                df.at[j, 'source'] == anchor_source):\n",
    "                \n",
    "                merged_lines.append(df.at[j, 'line'])\n",
    "                rows_to_merge.append(j)\n",
    "                j += 1\n",
    "            else:\n",
    "                # Stop if we hit a non-matching row\n",
    "                break\n",
    "        \n",
    "        # If we found rows to merge\n",
    "        if rows_to_merge:\n",
    "            # Merge all lines into the anchor row\n",
    "            df.at[i, 'line'] = ', '.join(merged_lines)\n",
    "            \n",
    "            # Handle other columns - merge non-NaN values\n",
    "            for col in df.columns:\n",
    "                if col not in ['line', 'source']:  # Skip already handled columns\n",
    "                    values = []\n",
    "                    # Start with anchor value\n",
    "                    if pd.notna(df.at[i, col]) and str(df.at[i, col]).strip():\n",
    "                        values.append(str(df.at[i, col]).strip())\n",
    "                    \n",
    "                    # Add values from rows to be merged\n",
    "                    for row_idx in rows_to_merge:\n",
    "                        if pd.notna(df.at[row_idx, col]) and str(df.at[row_idx, col]).strip():\n",
    "                            val = str(df.at[row_idx, col]).strip()\n",
    "                            if val not in values:  # Avoid duplicates\n",
    "                                values.append(val)\n",
    "                    \n",
    "                    # Set the merged value\n",
    "                    df.at[i, col] = ', '.join(values) if values else ''\n",
    "            \n",
    "            # Drop the merged rows and reset index\n",
    "            df = df.drop(rows_to_merge).reset_index(drop=True)\n",
    "            # Continue from the next row after the anchor (no increment needed as indices shifted)\n",
    "        else:\n",
    "            # No rows to merge, move to next row\n",
    "            i += 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the fixed function\n",
    "\n",
    "df_combined_working = merge_consecutive_b_rows_anchor(df_combined_working)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-25T20:34:19.773130Z",
     "start_time": "2025-10-25T20:34:15.140222Z"
    }
   },
   "id": "100cec2321cba061",
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "source": [
    "# More thorough deduplication matching your detection logic\n",
    "df_combined_working['akkadian_clean'] = df_combined_working['akkadian'].str.strip().str.lower()\n",
    "df_combined_working['english_clean'] = df_combined_working['english'].str.strip().str.lower()\n",
    "\n",
    "# Remove duplicates based on cleaned versions\n",
    "df_combined_working = df_combined_working.drop_duplicates(subset=['akkadian_clean', 'english_clean'])\n",
    "\n",
    "# Clean up temporary columns\n",
    "df_combined_working = df_combined_working.drop(['akkadian_clean', 'english_clean'], axis=1).dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-25T20:35:49.818734Z",
     "start_time": "2025-10-25T20:35:49.726142Z"
    }
   },
   "id": "93e20c17a72ec735",
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "source": [
    "df_combined_working = df_combined_working[(df_combined_working['akkadian'].str.split().str.len()> 2) &\n",
    "                            (df_combined_working['english'].str.split().str.len() > 2)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-25T20:35:50.916256Z",
     "start_time": "2025-10-25T20:35:50.722202Z"
    }
   },
   "id": "127af64b1328d879",
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "a\n",
    "## Saving the final results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b41cd6741921e39b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:35:55.514655Z",
     "start_time": "2025-10-25T20:35:55.319238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_combined_working.to_csv(\"data_directories/final_data/full_data_processed.csv\", index=False, encoding=\"utf-8\")\n",
    "df_combined_working['english'].to_csv('data_directories/final_data/english_sentences.txt', index=False, header=False, encoding=\"utf-8\")\n",
    "df_combined_working['akkadian'].to_csv('data_directories/final_data/akkadian_sentences.txt', index=False, header=False, encoding=\"utf-8\")"
   ],
   "id": "b2e3861293cc9554",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-25T20:35:56.967362Z",
     "start_time": "2025-10-25T20:35:56.957050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_combined_working = df_combined_working.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Extract test and validation sets (1000 each)\n",
    "test_data = df_combined_working.iloc[:1000]\n",
    "val_data = df_combined_working.iloc[1000:2000]\n",
    "train_data = df_combined_working.iloc[2000:]"
   ],
   "id": "84b1ba4928035695",
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "source": [
    "# Enhanced Data Leakage Detection + Automatic Cleanup\n",
    "\n",
    "# Check for overlapping English phrases\n",
    "train_en_set = set(train_data['english'].str.strip().str.lower())\n",
    "val_en_set = set(val_data['english'].str.strip().str.lower())\n",
    "test_en_set = set(test_data['english'].str.strip().str.lower())\n",
    "\n",
    "# Check for overlapping Akkadian phrases\n",
    "train_tr_set = set(train_data['akkadian'].str.strip().str.lower())\n",
    "val_tr_set = set(val_data['akkadian'].str.strip().str.lower())\n",
    "test_tr_set = set(test_data['akkadian'].str.strip().str.lower())\n",
    "\n",
    "# Check for overlapping complete pairs (English + Akkadian together)\n",
    "train_pairs = set(zip(train_data['english'].str.strip().str.lower(),\n",
    "                      train_data['akkadian'].str.strip().str.lower()))\n",
    "val_pairs = set(zip(val_data['english'].str.strip().str.lower(),\n",
    "                    val_data['akkadian'].str.strip().str.lower()))\n",
    "test_pairs = set(zip(test_data['english'].str.strip().str.lower(),\n",
    "                     test_data['akkadian'].str.strip().str.lower()))\n",
    "\n",
    "# Individual language intersections\n",
    "val_en_overlap = train_en_set.intersection(val_en_set)\n",
    "test_en_overlap = train_en_set.intersection(test_en_set)\n",
    "val_tr_overlap = train_tr_set.intersection(val_tr_set)\n",
    "test_tr_overlap = train_tr_set.intersection(test_tr_set)\n",
    "\n",
    "# Complete pair intersections\n",
    "val_pair_overlap = train_pairs.intersection(val_pairs)\n",
    "test_pair_overlap = train_pairs.intersection(test_pairs)\n",
    "\n",
    "# ORIGINAL Results Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"🔍 ORIGINAL DATA LEAKAGE DETECTION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n📝 INDIVIDUAL LANGUAGE OVERLAPS:\")\n",
    "print(f\"   English    → val/train = {len(val_en_overlap):3d}, test/train = {len(test_en_overlap):3d}\")\n",
    "print(f\"   Akkadian   → val/train = {len(val_tr_overlap):3d}, test/train = {len(test_tr_overlap):3d}\")\n",
    "\n",
    "print(f\"\\n🔗 COMPLETE PAIR OVERLAPS:\")\n",
    "print(f\"   Both langs → val/train = {len(val_pair_overlap):3d}, test/train = {len(test_pair_overlap):3d}\")\n",
    "\n",
    "# Severity assessment\n",
    "total_issues = len(val_en_overlap) + len(test_en_overlap) + len(val_tr_overlap) + len(test_tr_overlap) + len(val_pair_overlap) + len(test_pair_overlap)\n",
    "\n",
    "if total_issues == 0:\n",
    "    print(f\"\\n✅ STATUS: CLEAN - No data leakage detected!\")\n",
    "else:\n",
    "    print(f\"\\n⚠  STATUS: OVERLAP DETECTED - Will clean training data...\")\n",
    "\n",
    "# =============================================================================\n",
    "# AUTOMATIC CLEANUP: Remove overlapping rows from training data\n",
    "# =============================================================================\n",
    "\n",
    "if total_issues > 0:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🧹 CLEANING TRAINING DATA\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    original_train_size = len(train_data)\n",
    "\n",
    "    # Create normalized columns for matching\n",
    "    train_data_clean = train_data.copy()\n",
    "    train_data_clean['english_norm'] = train_data_clean['english'].str.strip().str.lower()\n",
    "    train_data_clean['akkadian_norm'] = train_data_clean['akkadian'].str.strip().str.lower()\n",
    "\n",
    "    # Collect all overlapping items to remove\n",
    "    all_overlapping_english = val_en_overlap.union(test_en_overlap)\n",
    "    all_overlapping_akkadian = val_tr_overlap.union(test_tr_overlap)\n",
    "    all_overlapping_pairs = val_pair_overlap.union(test_pair_overlap)\n",
    "\n",
    "    # Create boolean masks for rows to remove\n",
    "    english_mask = train_data_clean['english_norm'].isin(all_overlapping_english)\n",
    "    akkadian_mask = train_data_clean['akkadian_norm'].isin(all_overlapping_akkadian)\n",
    "\n",
    "    # For complete pairs, create a combined mask\n",
    "    pair_mask = pd.Series(False, index=train_data_clean.index)\n",
    "    if all_overlapping_pairs:\n",
    "        for en, ak in all_overlapping_pairs:\n",
    "            pair_condition = (train_data_clean['english_norm'] == en) & (train_data_clean['akkadian_norm'] == ak)\n",
    "            pair_mask = pair_mask | pair_condition\n",
    "\n",
    "    # Combine all masks (remove if ANY overlap detected)\n",
    "    rows_to_remove = english_mask | akkadian_mask | pair_mask\n",
    "\n",
    "    # Apply the cleanup\n",
    "    train_data_cleaned = train_data_clean[~rows_to_remove].drop(['english_norm', 'akkadian_norm'], axis=1)\n",
    "\n",
    "    removed_count = original_train_size - len(train_data_cleaned)\n",
    "\n",
    "    print(f\"📊 Training data size: {original_train_size:,} → {len(train_data_cleaned):,}\")\n",
    "    print(f\"🗑  Removed {removed_count:,} overlapping rows ({removed_count/original_train_size*100:.1f}%)\")\n",
    "\n",
    "    # Verify cleanup worked\n",
    "    print(f\"\\n🔍 VERIFYING CLEANUP...\")\n",
    "\n",
    "    # Re-run detection on cleaned data\n",
    "    train_en_clean = set(train_data_cleaned['english'].str.strip().str.lower())\n",
    "    train_tr_clean = set(train_data_cleaned['akkadian'].str.strip().str.lower())\n",
    "    train_pairs_clean = set(zip(train_data_cleaned['english'].str.strip().str.lower(),\n",
    "                                train_data_cleaned['akkadian'].str.strip().str.lower()))\n",
    "\n",
    "    # Check for remaining overlaps\n",
    "    val_en_clean = train_en_clean.intersection(val_en_set)\n",
    "    test_en_clean = train_en_clean.intersection(test_en_set)\n",
    "    val_tr_clean = train_tr_clean.intersection(val_tr_set)\n",
    "    test_tr_clean = train_tr_clean.intersection(test_tr_set)\n",
    "    val_pair_clean = train_pairs_clean.intersection(val_pairs)\n",
    "    test_pair_clean = train_pairs_clean.intersection(test_pairs)\n",
    "\n",
    "    remaining_issues = len(val_en_clean) + len(test_en_clean) + len(val_tr_clean) + len(test_tr_clean) + len(val_pair_clean) + len(test_pair_clean)\n",
    "\n",
    "    print(f\"\\n📝 CLEANED OVERLAPS:\")\n",
    "    print(f\"   English    → val/train = {len(val_en_clean):3d}, test/train = {len(test_en_clean):3d}\")\n",
    "    print(f\"   Akkadian   → val/train = {len(val_tr_clean):3d}, test/train = {len(test_tr_clean):3d}\")\n",
    "    print(f\"   Both langs → val/train = {len(val_pair_clean):3d}, test/train = {len(test_pair_clean):3d}\")\n",
    "\n",
    "    if remaining_issues == 0:\n",
    "        print(f\"\\n✅ SUCCESS: All overlaps removed!\")\n",
    "        print(f\"💾 Use 'train_data_cleaned' for training\")\n",
    "    else:\n",
    "        print(f\"\\n⚠  WARNING: {remaining_issues} overlaps still remain\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n💾 No cleanup needed - use original 'train_data'\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-25T20:35:57.705835Z",
     "start_time": "2025-10-25T20:35:57.450138Z"
    }
   },
   "id": "31bc3ba42a304747",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🔍 ORIGINAL DATA LEAKAGE DETECTION RESULTS\n",
      "============================================================\n",
      "\n",
      "📝 INDIVIDUAL LANGUAGE OVERLAPS:\n",
      "   English    → val/train =  76, test/train =  61\n",
      "   Akkadian   → val/train =  37, test/train =  40\n",
      "\n",
      "🔗 COMPLETE PAIR OVERLAPS:\n",
      "   Both langs → val/train =   0, test/train =   0\n",
      "\n",
      "⚠  STATUS: OVERLAP DETECTED - Will clean training data...\n",
      "\n",
      "============================================================\n",
      "🧹 CLEANING TRAINING DATA\n",
      "============================================================\n",
      "📊 Training data size: 32,690 → 32,260\n",
      "🗑  Removed 430 overlapping rows (1.3%)\n",
      "\n",
      "🔍 VERIFYING CLEANUP...\n",
      "\n",
      "📝 CLEANED OVERLAPS:\n",
      "   English    → val/train =   0, test/train =   0\n",
      "   Akkadian   → val/train =   0, test/train =   0\n",
      "   Both langs → val/train =   0, test/train =   0\n",
      "\n",
      "✅ SUCCESS: All overlaps removed!\n",
      "💾 Use 'train_data_cleaned' for training\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "source": [
    "# Create output directory if needed\n",
    "output_dir = \"data_directories/final_data/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save English and Akkadian splits\n",
    "train_data_cleaned['english'].to_csv(f\"{output_dir}/english_train.txt\", index=False, header=False, encoding='utf-8')\n",
    "train_data_cleaned['akkadian'].to_csv(f\"{output_dir}/akkadian_train.txt\", index=False, header=False, encoding='utf-8')\n",
    "\n",
    "val_data['english'].to_csv(f\"{output_dir}/english_val.txt\", index=False, header=False, encoding='utf-8')\n",
    "val_data['akkadian'].to_csv(f\"{output_dir}/akkadian_val.txt\", index=False, header=False, encoding='utf-8')\n",
    "\n",
    "test_data['english'].to_csv(f\"{output_dir}/english_test.txt\", index=False, header=False, encoding='utf-8')\n",
    "test_data['akkadian'].to_csv(f\"{output_dir}/akkadian_test.txt\", index=False, header=False, encoding='utf-8')\n",
    "\n",
    "print(\"✅ Split complete: train, val, test saved to\", output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-10-25T20:36:00.765445Z",
     "start_time": "2025-10-25T20:36:00.665639Z"
    }
   },
   "id": "62e438b9c78f8841",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split complete: train, val, test saved to data_directories/final_data/\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cross validation saving",
   "id": "5d5eb0774db8a538"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-29T17:47:01.132649Z",
     "start_time": "2025-06-29T17:46:58.218939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "df = df_combined_working.reset_index(drop=True)\n",
    "assert len(df) >= 3000, \"Dataset must have at least 3000 rows for this split logic.\"\n",
    "\n",
    "NUM_SPLITS = 5\n",
    "SPLIT_SIZE = 1000  # For test and val\n",
    "\n",
    "for fold in range(1, NUM_SPLITS + 1):\n",
    "    # Shuffle rows for this split (new split every time)\n",
    "    df_shuffled = df.sample(frac=1, random_state=fold).reset_index(drop=True)\n",
    "\n",
    "    # Select indices for test, val, train\n",
    "    test_indices = df_shuffled.index[:SPLIT_SIZE]\n",
    "    val_indices = df_shuffled.index[SPLIT_SIZE:SPLIT_SIZE*2]\n",
    "    train_indices = df_shuffled.index[SPLIT_SIZE*2:]\n",
    "\n",
    "    test_data = df_shuffled.loc[test_indices].reset_index(drop=True)\n",
    "    val_data = df_shuffled.loc[val_indices].reset_index(drop=True)\n",
    "    train_data = df_shuffled.loc[train_indices].reset_index(drop=True)\n",
    "\n",
    "    # === Data Leakage Detection and Cleanup (your logic, as before) ===\n",
    "    train_en_set = set(train_data['english'].str.strip().str.lower())\n",
    "    val_en_set = set(val_data['english'].str.strip().str.lower())\n",
    "    test_en_set = set(test_data['english'].str.strip().str.lower())\n",
    "\n",
    "    train_tr_set = set(train_data['akkadian'].str.strip().str.lower())\n",
    "    val_tr_set = set(val_data['akkadian'].str.strip().str.lower())\n",
    "    test_tr_set = set(test_data['akkadian'].str.strip().str.lower())\n",
    "\n",
    "    train_pairs = set(zip(train_data['english'].str.strip().str.lower(),\n",
    "                          train_data['akkadian'].str.strip().str.lower()))\n",
    "    val_pairs = set(zip(val_data['english'].str.strip().str.lower(),\n",
    "                        val_data['akkadian'].str.strip().str.lower()))\n",
    "    test_pairs = set(zip(test_data['english'].str.strip().str.lower(),\n",
    "                         test_data['akkadian'].str.strip().str.lower()))\n",
    "\n",
    "    val_en_overlap = train_en_set.intersection(val_en_set)\n",
    "    test_en_overlap = train_en_set.intersection(test_en_set)\n",
    "    val_tr_overlap = train_tr_set.intersection(val_tr_set)\n",
    "    test_tr_overlap = train_tr_set.intersection(test_tr_set)\n",
    "    val_pair_overlap = train_pairs.intersection(val_pairs)\n",
    "    test_pair_overlap = train_pairs.intersection(test_pairs)\n",
    "\n",
    "    total_issues = (len(val_en_overlap) + len(test_en_overlap) +\n",
    "                    len(val_tr_overlap) + len(test_tr_overlap) +\n",
    "                    len(val_pair_overlap) + len(test_pair_overlap))\n",
    "\n",
    "    # Automatic cleanup if overlaps detected\n",
    "    if total_issues > 0:\n",
    "        train_data_clean = train_data.copy()\n",
    "        train_data_clean['english_norm'] = train_data_clean['english'].str.strip().str.lower()\n",
    "        train_data_clean['akkadian_norm'] = train_data_clean['akkadian'].str.strip().str.lower()\n",
    "\n",
    "        all_overlapping_english = val_en_overlap.union(test_en_overlap)\n",
    "        all_overlapping_akkadian = val_tr_overlap.union(test_tr_overlap)\n",
    "        all_overlapping_pairs = val_pair_overlap.union(test_pair_overlap)\n",
    "\n",
    "        english_mask = train_data_clean['english_norm'].isin(all_overlapping_english)\n",
    "        akkadian_mask = train_data_clean['akkadian_norm'].isin(all_overlapping_akkadian)\n",
    "\n",
    "        pair_mask = pd.Series(False, index=train_data_clean.index)\n",
    "        if all_overlapping_pairs:\n",
    "            for en, ak in all_overlapping_pairs:\n",
    "                pair_condition = (train_data_clean['english_norm'] == en) & (train_data_clean['akkadian_norm'] == ak)\n",
    "                pair_mask = pair_mask | pair_condition\n",
    "\n",
    "        rows_to_remove = english_mask | akkadian_mask | pair_mask\n",
    "        train_data_cleaned = train_data_clean[~rows_to_remove].drop(['english_norm', 'akkadian_norm'], axis=1)\n",
    "    else:\n",
    "        train_data_cleaned = train_data\n",
    "\n",
    "    # === Save the splits ===\n",
    "    split_dir = f\"data_directories/final_data/split_{fold}\"\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "    train_data_cleaned['english'].to_csv(f\"{split_dir}/english_train.txt\", index=False, header=False, encoding='utf-8')\n",
    "    train_data_cleaned['akkadian'].to_csv(f\"{split_dir}/akkadian_train.txt\", index=False, header=False, encoding='utf-8')\n",
    "    val_data['english'].to_csv(f\"{split_dir}/english_val.txt\", index=False, header=False, encoding='utf-8')\n",
    "    val_data['akkadian'].to_csv(f\"{split_dir}/akkadian_val.txt\", index=False, header=False, encoding='utf-8')\n",
    "    test_data['english'].to_csv(f\"{split_dir}/english_test.txt\", index=False, header=False, encoding='utf-8')\n",
    "    test_data['akkadian'].to_csv(f\"{split_dir}/akkadian_test.txt\", index=False, header=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"✅ Fold {fold}: Train={len(train_data_cleaned)}, Val={len(val_data)}, Test={len(test_data)} saved to {split_dir}\")\n"
   ],
   "id": "499e9fb188e7d6ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fold 1: Train=38007, Val=1000, Test=1000 saved to data_directories/final_data/split_1\n",
      "✅ Fold 2: Train=38047, Val=1000, Test=1000 saved to data_directories/final_data/split_2\n",
      "✅ Fold 3: Train=38070, Val=1000, Test=1000 saved to data_directories/final_data/split_3\n",
      "✅ Fold 4: Train=38123, Val=1000, Test=1000 saved to data_directories/final_data/split_4\n",
      "✅ Fold 5: Train=38006, Val=1000, Test=1000 saved to data_directories/final_data/split_5\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
