{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Documentation\n",
    "Currently, we extract from the text the normalized form of the akkadain words, we extract words that do not have a normalized form \n",
    "\n",
    "we should omit all the captial letters from the text since they represent text that shouldn't be there"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38d3c9f254ad5c27"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-07T12:45:20.873946Z",
     "start_time": "2025-06-07T12:45:10.785570Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 212\u001B[0m\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parse_oracc_html(html_text)\n\u001B[1;32m    211\u001B[0m \u001B[38;5;66;03m# Run and print\u001B[39;00m\n\u001B[0;32m--> 212\u001B[0m parsed \u001B[38;5;241m=\u001B[39m \u001B[43mextract_oracc_translations_with_accumulation\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mribo.html\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime taken:\u001B[39m\u001B[38;5;124m\"\u001B[39m, time() \u001B[38;5;241m-\u001B[39m start)\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m parsed:\n",
      "Cell \u001B[0;32mIn[8], line 208\u001B[0m, in \u001B[0;36mextract_oracc_translations_with_accumulation\u001B[0;34m(html_path)\u001B[0m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(html_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[1;32m    207\u001B[0m     html_text \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m--> 208\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparse_oracc_html\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhtml_text\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[8], line 151\u001B[0m, in \u001B[0;36mparse_oracc_html\u001B[0;34m(html_text, base_url, filename)\u001B[0m\n\u001B[1;32m    149\u001B[0m akk_cell \u001B[38;5;241m=\u001B[39m row\u001B[38;5;241m.\u001B[39mselect_one(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtd.tlit\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m akk_cell:\n\u001B[0;32m--> 151\u001B[0m     akk_text, line_unlinked_data \u001B[38;5;241m=\u001B[39m \u001B[43mextract_normalized_akkadian_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43makk_cell\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbase_url\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    153\u001B[0m     akk_text, line_unlinked_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m, []\n",
      "Cell \u001B[0;32mIn[8], line 87\u001B[0m, in \u001B[0;36mextract_normalized_akkadian_text\u001B[0;34m(cell, base_url)\u001B[0m\n\u001B[1;32m     85\u001B[0m encoded_sig \u001B[38;5;241m=\u001B[39m urllib\u001B[38;5;241m.\u001B[39mparse\u001B[38;5;241m.\u001B[39mquote(sig, safe\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     86\u001B[0m sig_url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbase_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mproject\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m?sig=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mencoded_sig\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 87\u001B[0m normalized \u001B[38;5;241m=\u001B[39m \u001B[43mget_first_normalized_form_from_link\u001B[49m\u001B[43m(\u001B[49m\u001B[43msig_url\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m normalized:\n\u001B[1;32m     89\u001B[0m     words\u001B[38;5;241m.\u001B[39mappend(normalized)\n",
      "Cell \u001B[0;32mIn[8], line 40\u001B[0m, in \u001B[0;36mget_first_normalized_form_from_link\u001B[0;34m(sig_url)\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_first_normalized_form_from_link\u001B[39m(sig_url):\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 40\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43msig_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m200\u001B[39m:\n\u001B[1;32m     42\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/gpu_no_rapids/lib/python3.10/site-packages/requests/api.py:73\u001B[0m, in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     63\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/gpu_no_rapids/lib/python3.10/site-packages/requests/api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/gpu_no_rapids/lib/python3.10/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/anaconda3/envs/gpu_no_rapids/lib/python3.10/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m~/anaconda3/envs/gpu_no_rapids/lib/python3.10/site-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    787\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    789\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 790\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    803\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    805\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[1;32m    806\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:467\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    464\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    465\u001B[0m     \u001B[38;5;66;03m# Trigger any extra validation we need to do.\u001B[39;00m\n\u001B[1;32m    466\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 467\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    468\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    469\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_raise_timeout(err\u001B[38;5;241m=\u001B[39me, url\u001B[38;5;241m=\u001B[39murl, timeout_value\u001B[38;5;241m=\u001B[39mconn\u001B[38;5;241m.\u001B[39mtimeout)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connectionpool.py:1096\u001B[0m, in \u001B[0;36mHTTPSConnectionPool._validate_conn\u001B[0;34m(self, conn)\u001B[0m\n\u001B[1;32m   1094\u001B[0m \u001B[38;5;66;03m# Force connect early to allow us to validate the connection.\u001B[39;00m\n\u001B[1;32m   1095\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_closed:\n\u001B[0;32m-> 1096\u001B[0m     \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1098\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_verified:\n\u001B[1;32m   1099\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1100\u001B[0m         (\n\u001B[1;32m   1101\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnverified HTTPS request is being made to host \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconn\u001B[38;5;241m.\u001B[39mhost\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1106\u001B[0m         InsecureRequestWarning,\n\u001B[1;32m   1107\u001B[0m     )\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:611\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    610\u001B[0m     sock: socket\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m|\u001B[39m ssl\u001B[38;5;241m.\u001B[39mSSLSocket\n\u001B[0;32m--> 611\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m sock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    612\u001B[0m     server_hostname: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n\u001B[1;32m    613\u001B[0m     tls_in_tls \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/connection.py:203\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001B[39;00m\n\u001B[1;32m    199\u001B[0m \n\u001B[1;32m    200\u001B[0m \u001B[38;5;124;03m:return: New socket connection.\u001B[39;00m\n\u001B[1;32m    201\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 203\u001B[0m     sock \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_connection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dns_host\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43msource_address\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msource_address\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43msocket_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    209\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    210\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NameResolutionError(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost, \u001B[38;5;28mself\u001B[39m, e) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m source_address:\n\u001B[1;32m     72\u001B[0m     sock\u001B[38;5;241m.\u001B[39mbind(source_address)\n\u001B[0;32m---> 73\u001B[0m \u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43msa\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n\u001B[1;32m     75\u001B[0m err \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib3\n",
    "import urllib.parse\n",
    "from time import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "start = time()\n",
    "# Disable SSL warnings — only for dev/testing!\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def clean_visible_text(text):\n",
    "    text = text.strip()\n",
    "\n",
    "    # ✅ Step 0: Reject leading garbage like \"...]-\" or \"[x]-\"\n",
    "    if re.match(r\"^.*[\\[\\](){}<>]+.*\\]-+\", text):\n",
    "        return None\n",
    "\n",
    "    # ✅ Step 1: Strip trailing \"-[...]\" or similar\n",
    "    text = re.sub(r\"-+[\\[\\](){}<>].*$\", \"\", text).strip()\n",
    "\n",
    "    # ✅ Step 2: Exclude pure garbage: only x, brackets, punctuation\n",
    "    if re.fullmatch(r\"[xX\\[\\](){}<>.\\- ?]+\", text):\n",
    "        return None\n",
    "\n",
    "    # ✅ Step 3: Exclude anything that still contains bracketed content\n",
    "    if re.search(r\"[\\[\\](){}<>].*[\\]\\})>]\", text):\n",
    "        return None\n",
    "\n",
    "    # ✅ Step 4: Final sanity check\n",
    "    if not text or re.fullmatch(r\"[xX\\-\\s]+\", text):\n",
    "        return None\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_first_normalized_form_from_link(sig_url):\n",
    "    try:\n",
    "        response = requests.get(sig_url, verify=False)\n",
    "        if response.status_code != 200:\n",
    "            return None\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Look for a.norms a.icountu that has plain text content (not spans)\n",
    "        for a in soup.select(\"p.norms a.icountu\"):\n",
    "            if a.find(\"span\") is None:  # ✅ not a sign group\n",
    "                text = a.get_text(strip=True)\n",
    "                if text:\n",
    "                    return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving normalized form from {sig_url}: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_project_from_sig(sig):\n",
    "    if sig.startswith(\"☣@\"):\n",
    "        try:\n",
    "            full_path = sig.split(\"@\")[1].split(\"%\")[0]\n",
    "            return full_path.split(\"/\")[0]\n",
    "        except IndexError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_normalized_akkadian_text(cell, base_url):\n",
    "    words = []\n",
    "    line_unlinked_data = []  # Store unlinked data for this specific akkadian line\n",
    "\n",
    "    for word_span in cell.select(\"span.w[class*='akk']\"):\n",
    "        link = word_span.find(\"a\", class_=\"cbd\")\n",
    "        used = False\n",
    "\n",
    "        # ✅ Condition: skip ? only if no link\n",
    "        sup = word_span.select_one(\"sup\")\n",
    "        if sup and \"?\" in sup.get_text(strip=True) and not link:\n",
    "            continue  # ❌ skip uncertain unlinked words\n",
    "\n",
    "        # Try linked normalization\n",
    "        if link and link.has_attr(\"data-wsig\"):\n",
    "            sig = link[\"data-wsig\"]\n",
    "            project = extract_project_from_sig(sig)\n",
    "            if project:\n",
    "                encoded_sig = urllib.parse.quote(sig, safe='')\n",
    "                sig_url = f\"{base_url}/{project}?sig={encoded_sig}\"\n",
    "                normalized = get_first_normalized_form_from_link(sig_url)\n",
    "                if normalized:\n",
    "                    words.append(normalized)\n",
    "                    used = True\n",
    "\n",
    "        # Try title-based normalization\n",
    "        if not used:\n",
    "            title = word_span.get(\"title\", \"\")\n",
    "            if \"$\" in title:\n",
    "                norm_forms = title.split(\"$\")[1:]\n",
    "                if norm_forms:\n",
    "                    first_norm = norm_forms[0].strip()\n",
    "                    if first_norm:\n",
    "                        words.append(first_norm)\n",
    "                        used = True\n",
    "\n",
    "        # ✅ Modified: For unlinked data, use marker and store actual text\n",
    "        if not used:\n",
    "            raw_text = word_span.get_text(strip=True)\n",
    "            if raw_text:\n",
    "                words.append(\"<data>\")  # Add marker instead of actual text\n",
    "                line_unlinked_data.append(raw_text)  # Store raw text without cleaning\n",
    "\n",
    "    return ' '.join(words), line_unlinked_data\n",
    "\n",
    "\n",
    "def extract_english_text(cell):\n",
    "    return ' '.join(span.get_text(strip=True) for span in cell.select(\"span.w\"))\n",
    "\n",
    "\n",
    "def save_unlinked_data_to_file(unlinked_data_store, filename=\"unlinked_data.txt\"):\n",
    "    \"\"\"Save unlinked data to file, organized by line number\"\"\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        for line_number in unlinked_data_store.keys():\n",
    "            data_pieces = unlinked_data_store[line_number]\n",
    "            joined_data = \".,.\".join(data_pieces)\n",
    "            file.write(f\"{line_number}: {joined_data}\\n\")\n",
    "    print(f\"Unlinked data saved to {filename}\")\n",
    "\n",
    "\n",
    "def parse_oracc_html(html_text, base_url=\"https://oracc.museum.upenn.edu\", filename=\"unlinked_data.txt\"):\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    results = []\n",
    "    accumulated_akkadian = \"\"\n",
    "    accumulated_unlinked_data = []  # Accumulate unlinked data like akkadian text\n",
    "    unlinked_data_store = {}  # Dictionary to store unlinked data by English line number\n",
    "    table_rows = soup.select_one(\"table.transliteration\") or soup.select_one(\"table.composite\")\n",
    "    if not table_rows:\n",
    "        print(\"No transliteration or composite rows found. Check table class.\")\n",
    "    table_rows = table_rows.select(\"tr\")\n",
    "    for row in table_rows:\n",
    "        line_number_tag = row.select_one(\"td.lnum .lnum\")\n",
    "        line_number = line_number_tag.get_text(strip=True) if line_number_tag else \"\"\n",
    "\n",
    "        eng_cell = row.select_one(\"td.xtr\")\n",
    "        eng_label_tag = row.select_one(\"td.xtr span.xtr-label\")\n",
    "        eng_line_number = (\n",
    "            eng_label_tag.get_text(strip=True).strip(\"()\") if eng_label_tag else \"\"\n",
    "        )\n",
    "\n",
    "        eng_text = extract_english_text(eng_cell) if eng_cell else \"\"\n",
    "\n",
    "        akk_cell = row.select_one(\"td.tlit\")\n",
    "        if akk_cell:\n",
    "            akk_text, line_unlinked_data = extract_normalized_akkadian_text(akk_cell, base_url)\n",
    "        else:\n",
    "            akk_text, line_unlinked_data = \"\", []\n",
    "\n",
    "        if not eng_text:\n",
    "            # Accumulate both akkadian text and unlinked data\n",
    "            accumulated_akkadian += \" \" + akk_text\n",
    "            accumulated_unlinked_data.extend(line_unlinked_data)\n",
    "        else:\n",
    "            if results:\n",
    "                # Add accumulated data to the previous entry\n",
    "                results[-1][\"akkadian\"] += \" \" + accumulated_akkadian.strip()\n",
    "                # Store accumulated unlinked data for the previous English line\n",
    "                if accumulated_unlinked_data:\n",
    "                    prev_line = results[-1][\"line\"]\n",
    "                    if prev_line in unlinked_data_store:\n",
    "                        unlinked_data_store[prev_line].extend(accumulated_unlinked_data)\n",
    "                    else:\n",
    "                        unlinked_data_store[prev_line] = accumulated_unlinked_data[:]\n",
    "                accumulated_akkadian = \"\"\n",
    "                accumulated_unlinked_data = []\n",
    "\n",
    "            entry = {\n",
    "                \"line\": eng_line_number or line_number,\n",
    "                \"akkadian\": akk_text,\n",
    "                \"english\": eng_text,\n",
    "            }\n",
    "            results.append(entry)\n",
    "\n",
    "            # Store unlinked data for current English line\n",
    "            if line_unlinked_data:\n",
    "                current_line = entry[\"line\"]\n",
    "                if current_line in unlinked_data_store:\n",
    "                    unlinked_data_store[current_line].extend(line_unlinked_data)\n",
    "                else:\n",
    "                    unlinked_data_store[current_line] = line_unlinked_data[:]\n",
    "\n",
    "    if accumulated_akkadian and results:\n",
    "        results[-1][\"akkadian\"] += \" \" + accumulated_akkadian.strip()\n",
    "        # Handle final accumulated unlinked data\n",
    "        if accumulated_unlinked_data:\n",
    "            last_line = results[-1][\"line\"]\n",
    "            if last_line in unlinked_data_store:\n",
    "                unlinked_data_store[last_line].extend(accumulated_unlinked_data)\n",
    "            else:\n",
    "                unlinked_data_store[last_line] = accumulated_unlinked_data[:]\n",
    "\n",
    "    # Save unlinked data to file\n",
    "    if unlinked_data_store:\n",
    "        save_unlinked_data_to_file(unlinked_data_store, filename)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_oracc_translations_with_accumulation(html_path):\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_text = file.read()\n",
    "    return parse_oracc_html(html_text)\n",
    "\n",
    "\n",
    "# Run and print\n",
    "parsed = extract_oracc_translations_with_accumulation(\"ribo.html\")\n",
    "print(\"time taken:\", time() - start)\n",
    "for line in parsed:\n",
    "    print(f\"{line['line']}\")\n",
    "    print(f\"Akkadian: {line['akkadian']}\")\n",
    "    print(f\"English: {line['english']}\")\n",
    "    print(\"---\" * 15)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import concurrent.futures\n",
    "from threading import Lock\n",
    "from queue import Queue\n",
    "import threading\n",
    "import traceback\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Import your parsing functions\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib3\n",
    "import urllib.parse\n",
    "import re\n",
    "\n",
    "# Global lock for directory operations\n",
    "dir_lock = Lock()\n",
    "\n",
    "# Setup focused logging\n",
    "def setup_logging(log_file=\"scraper_results.log\"):\n",
    "    \"\"\"Setup focused logging for results and errors only\"\"\"\n",
    "    # Create formatter for clean output\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # File handler for detailed logs\n",
    "    file_handler = logging.FileHandler(log_file, encoding='utf-8')\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Console handler for important messages only\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(logging.WARNING)  # Only warnings and errors to console\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    # Setup logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers.clear()  # Clear existing handlers\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "def dir_checker(dir_path):\n",
    "    \"\"\"Thread-safe directory checker\"\"\"\n",
    "    with dir_lock:\n",
    "        if os.path.exists(dir_path):\n",
    "            shutil.rmtree(dir_path)\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "def get_chrome_driver():\n",
    "    \"\"\"Create a Chrome driver with optimized options\"\"\"\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--disable-images\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-plugins\")\n",
    "    chrome_options.add_argument(\"--disable-background-timer-throttling\")\n",
    "    chrome_options.add_argument(\"--disable-renderer-backgrounding\")\n",
    "    chrome_options.add_argument(\"--disable-backgrounding-occluded-windows\")\n",
    "    chrome_options.add_argument(\"--memory-pressure-off\")\n",
    "    chrome_options.add_argument(\"--max_old_space_size=4096\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    chrome_options.page_load_strategy = 'normal'\n",
    "\n",
    "    return webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def detect_page_content_type(driver):\n",
    "    \"\"\"Detect what type of content is available on the page\"\"\"\n",
    "    content_info = {\n",
    "        'transliteration_tables': 0,\n",
    "        'composite_tables': 0,\n",
    "        'total_tables': 0,\n",
    "        'has_content': False,\n",
    "        'page_type': 'unknown'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Check for transliteration tables\n",
    "        transliteration_tables = driver.find_elements(By.CSS_SELECTOR, \"table.transliteration\")\n",
    "        content_info['transliteration_tables'] = len(transliteration_tables)\n",
    "\n",
    "        # Check for composite tables\n",
    "        composite_tables = driver.find_elements(By.CSS_SELECTOR, \"table.composite\")\n",
    "        content_info['composite_tables'] = len(composite_tables)\n",
    "\n",
    "        # Check for any tables\n",
    "        all_tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "        content_info['total_tables'] = len(all_tables)\n",
    "\n",
    "        # Determine page type and content availability\n",
    "        if transliteration_tables:\n",
    "            content_info['page_type'] = 'transliteration'\n",
    "            content_info['has_content'] = True\n",
    "        elif composite_tables:\n",
    "            content_info['page_type'] = 'composite'\n",
    "            content_info['has_content'] = True\n",
    "        elif all_tables:\n",
    "            content_info['page_type'] = 'generic_table'\n",
    "            content_info['has_content'] = True\n",
    "        else:\n",
    "            content_info['page_type'] = 'no_tables'\n",
    "            content_info['has_content'] = False\n",
    "\n",
    "        # Additional content checks\n",
    "        if not content_info['has_content']:\n",
    "            content_containers = driver.find_elements(By.CSS_SELECTOR,\n",
    "                                                      \"#content, .content, #main, .main, .transliteration, .composite\")\n",
    "            if content_containers:\n",
    "                content_info['has_content'] = True\n",
    "                content_info['page_type'] = 'content_container'\n",
    "\n",
    "        return content_info\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Content detection error: {e}\")\n",
    "        return content_info\n",
    "\n",
    "def wait_for_page_load(driver, timeout=10):\n",
    "    \"\"\"Wait for page to fully load with minimal logging\"\"\"\n",
    "    try:\n",
    "        # Wait for document ready state\n",
    "        WebDriverWait(driver, timeout).until(\n",
    "            lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "        )\n",
    "\n",
    "        # Detect content type\n",
    "        content_info = detect_page_content_type(driver)\n",
    "\n",
    "        # Wait for specific content based on type\n",
    "        if content_info['has_content']:\n",
    "            if content_info['page_type'] == 'transliteration':\n",
    "                try:\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"table.transliteration\"))\n",
    "                    )\n",
    "                except TimeoutException:\n",
    "                    pass  # Continue anyway\n",
    "\n",
    "            elif content_info['page_type'] == 'composite':\n",
    "                try:\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"table.composite\"))\n",
    "                    )\n",
    "                except TimeoutException:\n",
    "                    pass  # Continue anyway\n",
    "\n",
    "            elif content_info['page_type'] == 'generic_table':\n",
    "                try:\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
    "                    )\n",
    "                except TimeoutException:\n",
    "                    pass  # Continue anyway\n",
    "\n",
    "        return content_info['has_content']\n",
    "\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def validate_page_content(driver, html_content):\n",
    "    \"\"\"Validate that the page contains parseable content\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        table_rows = soup.select_one(\"table.transliteration\") or soup.select_one(\"table.composite\")\n",
    "\n",
    "        if table_rows:\n",
    "            return True\n",
    "        else:\n",
    "            # Check for any tables for debugging\n",
    "            all_tables = soup.select(\"table\")\n",
    "            if all_tables:\n",
    "                logger.warning(f\"Found {len(all_tables)} tables but none with expected classes\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Content validation error: {e}\")\n",
    "        return False\n",
    "\n",
    "def scrape_single_page_enhanced(page_info, parse_function, dir_output, website_name, max_retries=3):\n",
    "    \"\"\"Enhanced single page scraper with focused logging\"\"\"\n",
    "    url, page_index = page_info\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        driver = None\n",
    "        try:\n",
    "            print(f\"Loading page {page_index}\")\n",
    "\n",
    "            driver = get_chrome_driver()\n",
    "            driver.set_page_load_timeout(60)\n",
    "            driver.implicitly_wait(10)\n",
    "\n",
    "            # Navigate to page\n",
    "            driver.get(url)\n",
    "\n",
    "            # Wait for page load\n",
    "            if not wait_for_page_load(driver):\n",
    "                raise TimeoutException(f\"Page failed to load within timeout\")\n",
    "\n",
    "            # Get page source and validate content\n",
    "            html = driver.page_source\n",
    "            if not validate_page_content(driver, html):\n",
    "                raise ValueError(\"Page contains no parseable content\")\n",
    "\n",
    "            # Parse the HTML content\n",
    "            try:\n",
    "                page_results = parse_function(html, filename=f\"{dir_output}/unlinked_data_page_{page_index}.txt\")\n",
    "\n",
    "                if not page_results:\n",
    "                    raise ValueError(\"Parser returned no results\")\n",
    "\n",
    "                logger.info(f\"SUCCESS - Page {page_index}: Parsed {len(page_results)} items\")\n",
    "\n",
    "            except Exception as parse_error:\n",
    "                # Save HTML for debugging parse issues\n",
    "                error_html_filename = f\"{dir_output}/error_page_{page_index}.html\"\n",
    "                os.makedirs(dir_output, exist_ok=True)\n",
    "                with open(error_html_filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(html)\n",
    "\n",
    "                logger.error(f\"PARSE ERROR - Page {page_index}: {str(parse_error)} | HTML saved to {error_html_filename}\")\n",
    "                raise parse_error\n",
    "\n",
    "            # Save page data\n",
    "            os.makedirs(dir_output, exist_ok=True)\n",
    "            df = pd.DataFrame(page_results)\n",
    "            df['source'] = driver.current_url\n",
    "            df.to_csv(f\"{dir_output}/data_{page_index}.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "            return {\n",
    "                'page_index': page_index,\n",
    "                'url': url,\n",
    "                'actual_url': driver.current_url,\n",
    "                'results': page_results,\n",
    "                'success': True,\n",
    "                'error': None,\n",
    "                'attempts': attempt + 1\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            error_type = type(e).__name__\n",
    "            error_msg = str(e)\n",
    "\n",
    "            if attempt < max_retries - 1:\n",
    "                logger.warning(f\"RETRY - Page {page_index} attempt {attempt + 1}: {error_type} - {error_msg}\")\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                logger.error(f\"FAILED - Page {page_index} after {max_retries} attempts: {error_type} - {error_msg}\")\n",
    "\n",
    "                # Save detailed error info for failed pages\n",
    "                error_details = {\n",
    "                    'page_index': page_index,\n",
    "                    'url': url,\n",
    "                    'error': error_msg,\n",
    "                    'error_type': error_type,\n",
    "                    'attempts': max_retries\n",
    "                }\n",
    "\n",
    "                return {\n",
    "                    'page_index': page_index,\n",
    "                    'url': url,\n",
    "                    'actual_url': driver.current_url if driver else None,\n",
    "                    'results': [],\n",
    "                    'success': False,\n",
    "                    'error': error_details,\n",
    "                    'attempts': max_retries\n",
    "                }\n",
    "\n",
    "        finally:\n",
    "            if driver:\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "def discover_all_pages_enhanced(start_url, max_discovery_pages=1e5, restart_interval=10):\n",
    "    \"\"\"Enhanced page discovery with focused logging\"\"\"\n",
    "    print(f\"Discovering pages from: {start_url}\")\n",
    "    logger.info(f\"DISCOVERY START: {start_url}\")\n",
    "\n",
    "    page_urls = []\n",
    "    page_index = 1\n",
    "    current_url = start_url\n",
    "    driver = None\n",
    "\n",
    "    while page_index <= max_discovery_pages:\n",
    "        # Restart Chrome periodically\n",
    "        if (page_index - 1) % restart_interval == 0:\n",
    "            if driver:\n",
    "                try:\n",
    "                    driver.quit()\n",
    "                except:\n",
    "                    pass\n",
    "            driver = get_chrome_driver()\n",
    "            driver.set_page_load_timeout(60)\n",
    "\n",
    "        try:\n",
    "            print(f\"Discovering page {page_index}\")\n",
    "            driver.get(current_url)\n",
    "\n",
    "            # Wait for page load and validate content\n",
    "            page_load_success = wait_for_page_load(driver)\n",
    "            actual_url = driver.current_url\n",
    "            html_content = driver.page_source\n",
    "            has_parseable_content = validate_page_content(driver, html_content)\n",
    "\n",
    "            if has_parseable_content:\n",
    "                page_urls.append((actual_url, page_index))\n",
    "            else:\n",
    "                logger.warning(f\"Page {page_index} has no parseable content\")\n",
    "                page_urls.append((actual_url, page_index))  # Still add it\n",
    "\n",
    "            # Look for next button\n",
    "            next_buttons = driver.find_elements(By.CSS_SELECTOR, \"img#p4ItemNext\")\n",
    "            if not next_buttons:\n",
    "                next_buttons = driver.find_elements(By.ID, \"p4ItemNext\")\n",
    "\n",
    "            if not next_buttons or not next_buttons[0].is_enabled():\n",
    "                logger.info(f\"Discovery complete: Found {len(page_urls)} total pages\")\n",
    "                break\n",
    "\n",
    "            # Click next and wait for page change\n",
    "            old_url = driver.current_url\n",
    "            try:\n",
    "                driver.execute_script(\"arguments[0].click();\", next_buttons[0])\n",
    "            except Exception:\n",
    "                try:\n",
    "                    next_buttons[0].click()\n",
    "                except Exception:\n",
    "                    logger.error(f\"Cannot click next button at page {page_index}\")\n",
    "                    break\n",
    "\n",
    "            # Wait for URL to change\n",
    "            start_time = time.time()\n",
    "            while driver.current_url == old_url and time.time() - start_time < 15:\n",
    "                time.sleep(0.25)\n",
    "\n",
    "            if driver.current_url == old_url:\n",
    "                logger.info(f\"Discovery end: No more pages (URL unchanged)\")\n",
    "                break\n",
    "\n",
    "            current_url = driver.current_url\n",
    "            page_index += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Discovery error at page {page_index}: {str(e)}\")\n",
    "            break\n",
    "\n",
    "    # Clean up\n",
    "    if driver:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    print(f\"Found {len(page_urls)} pages\")\n",
    "    logger.info(f\"Discovery result: {len(page_urls)} pages found\")\n",
    "    return page_urls\n",
    "\n",
    "def scrape_website_parallel_pages(start_url, dir_output, parse_function, max_workers=None):\n",
    "    \"\"\"Enhanced website scraper with focused logging\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = min(cpu_count(), 4)\n",
    "\n",
    "    # Create output directory\n",
    "    dir_checker(dir_output)\n",
    "\n",
    "    # Setup logging for this website\n",
    "    logger_path = os.path.join(dir_output, \"scraper_results.log\")\n",
    "    global logger\n",
    "    logger = setup_logging(logger_path)\n",
    "\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Scraping start: {dir_output} with {max_workers} workers\")\n",
    "\n",
    "    # Step 1: Discover all pages\n",
    "    page_urls = discover_all_pages_enhanced(start_url)\n",
    "\n",
    "    if not page_urls:\n",
    "        logger.error(f\"No pages found: {start_url}\")\n",
    "        return {\n",
    "            'dir_output': dir_output,\n",
    "            'total_pages': 0,\n",
    "            'successful_pages': 0,\n",
    "            'failed_pages': 0,\n",
    "            'total_items': 0,\n",
    "            'results': [],\n",
    "            'success': False\n",
    "        }\n",
    "\n",
    "    print(f\"Processing {len(page_urls)} pages with {max_workers} workers\")\n",
    "    logger.info(f\"Processing {len(page_urls)} pages with {max_workers} workers\")\n",
    "\n",
    "    # Step 2: Process pages in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        scrape_func = partial(scrape_single_page_enhanced,\n",
    "                              parse_function=parse_function,\n",
    "                              dir_output=dir_output,\n",
    "                              website_name=dir_output)\n",
    "\n",
    "        future_to_page = {executor.submit(scrape_func, page_info): page_info for page_info in page_urls}\n",
    "\n",
    "        page_results = []\n",
    "        for future in concurrent.futures.as_completed(future_to_page):\n",
    "            page_info = future_to_page[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                page_results.append(result)\n",
    "\n",
    "            except Exception as exc:\n",
    "                logger.error(f\"Executor error: Page {page_info[1]} - {str(exc)}\")\n",
    "                page_results.append({\n",
    "                    'page_index': page_info[1],\n",
    "                    'url': page_info[0],\n",
    "                    'actual_url': None,\n",
    "                    'results': [],\n",
    "                    'success': False,\n",
    "                    'error': {'error': str(exc), 'error_type': type(exc).__name__},\n",
    "                    'attempts': 0\n",
    "                })\n",
    "\n",
    "    # Combine results and create summary\n",
    "    all_results = []\n",
    "    error_pages = []\n",
    "    successful_pages = 0\n",
    "\n",
    "    for result in page_results:\n",
    "        if result['success']:\n",
    "            all_results.extend(result['results'])\n",
    "            successful_pages += 1\n",
    "        else:\n",
    "            error_pages.append({\n",
    "                'page_index': result['page_index'],\n",
    "                'url': result['url'],\n",
    "                'actual_url': result.get('actual_url'),\n",
    "                'error': result['error'],\n",
    "                'attempts': result.get('attempts', 0)\n",
    "            })\n",
    "\n",
    "    # Save detailed error log\n",
    "    if error_pages:\n",
    "        error_df = pd.DataFrame(error_pages)\n",
    "        error_df.to_csv(f\"{dir_output}/failed_pages.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        logger.error(f\"Failed pages: {len(error_pages)} pages failed (see failed_pages.csv)\")\n",
    "\n",
    "    # Save combined results\n",
    "    if all_results:\n",
    "        combined_df = pd.DataFrame(all_results)\n",
    "        combined_df.to_csv(f\"{dir_output}/combined_data.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    failed_pages = len(page_results) - successful_pages\n",
    "\n",
    "    # Final summary\n",
    "    print(f\"Completed {dir_output}: {successful_pages}/{len(page_urls)} pages successful\")\n",
    "    logger.info(f\"Scraping complete: {dir_output}\")\n",
    "    logger.info(f\"Results - Total: {len(page_urls)}, Successful: {successful_pages}, Failed: {failed_pages}, Items: {len(all_results)}, Time: {total_time:.2f}s\")\n",
    "\n",
    "    return {\n",
    "        'dir_output': dir_output,\n",
    "        'total_pages': len(page_urls),\n",
    "        'successful_pages': successful_pages,\n",
    "        'failed_pages': failed_pages,\n",
    "        'total_items': len(all_results),\n",
    "        'results': all_results,\n",
    "        'success': successful_pages > 0,\n",
    "        'processing_time': total_time,\n",
    "        'error_details': error_pages\n",
    "    }\n",
    "\n",
    "def scrape_multiple_websites_sequential(url_dir_pairs, parse_function, max_workers_per_site=None):\n",
    "    \"\"\"Scrape multiple websites sequentially, but with parallel page processing within each site\"\"\"\n",
    "    if max_workers_per_site is None:\n",
    "        max_workers_per_site = min(cpu_count(), 4)\n",
    "\n",
    "    total_start_time = time.time()\n",
    "    all_website_results = []\n",
    "\n",
    "    for i, (url, dir_output) in enumerate(url_dir_pairs, 1):\n",
    "        print(f\"\\nProcessing website {i}/{len(url_dir_pairs)}: {dir_output}\")\n",
    "\n",
    "        website_result = scrape_website_parallel_pages(\n",
    "            url, dir_output, parse_function, max_workers_per_site\n",
    "        )\n",
    "\n",
    "        all_website_results.append(website_result)\n",
    "\n",
    "    return all_website_results\n",
    "\n",
    "def main(parse_oracc_html):\n",
    "    \"\"\"Main function to run the scraping process\"\"\"\n",
    "    # Define your URLs and output directories\n",
    "    url_suhu = \"https://oracc.museum.upenn.edu/suhu/Q006211\"\n",
    "    url_saao = \"https://oracc.museum.upenn.edu/saao/P224485\"\n",
    "    url_rinap = \"https://oracc.museum.upenn.edu/rinap/Q006333?lang=en\"\n",
    "    url_riao = \"https://oracc.museum.upenn.edu/riao/Q005738\"\n",
    "    url_ribo = \"https://oracc.museum.upenn.edu/ribo/Q006263\"\n",
    "\n",
    "    # Create list of (url, directory) pairs\n",
    "    url_dir_pairs = [\n",
    "        (url_saao, \"saao\"),\n",
    "        (url_suhu, \"suhu\"),\n",
    "        (url_rinap, \"rinap\"),\n",
    "        (url_riao, \"raio\"),\n",
    "        (url_ribo, \"ribo\"),\n",
    "    ]\n",
    "\n",
    "    # Run the scraping process\n",
    "    results = scrape_multiple_websites_sequential(\n",
    "        url_dir_pairs,\n",
    "        parse_oracc_html,\n",
    "        max_workers_per_site=8\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from your_parsing_module import parse_oracc_html\n",
    "    results = main(parse_oracc_html)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea0386e45a8eb30a",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## File Splitter Tool",
   "id": "cfe9b26c83f3025a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## File Splitter Tool\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def split_files_by_type(source_dir, dry_run=True):\n",
    "    \"\"\"\n",
    "    Split files into 5 different directories based on their type\n",
    "\n",
    "    Args:\n",
    "        source_dir: Path to directory containing mixed files\n",
    "        dry_run: If True, only show what would be moved without actually doing it\n",
    "    \"\"\"\n",
    "\n",
    "    source_path = Path(source_dir)\n",
    "\n",
    "    if not source_path.exists():\n",
    "        print(f\"❌ Directory {source_dir} does not exist!\")\n",
    "        return\n",
    "\n",
    "    print(f\"🔍 Scanning directory: {source_path}\")\n",
    "    print(f\"🔄 Dry run mode: {'ON' if dry_run else 'OFF'}\")\n",
    "    print(\"─\" * 60)\n",
    "\n",
    "    # File patterns to match\n",
    "    patterns = {\n",
    "        'data': r'^data_(\\d+)\\.csv$',\n",
    "        'unlinked': r'^unlinked_data_page_(\\d+)\\.txt$',\n",
    "        'failed': r'^failed_pages\\.csv$',\n",
    "        'log': r'^scraper_results\\.log$',\n",
    "        'error': r'^error_page_(\\d+)\\.html$'\n",
    "    }\n",
    "\n",
    "    # Target directories\n",
    "    target_dirs = {\n",
    "        'data': source_path / 'data_files',\n",
    "        'unlinked': source_path / 'unlinked_files',\n",
    "        'failed': source_path / 'failed_files',\n",
    "        'log': source_path / 'log_files',\n",
    "        'error': source_path / 'error_files'\n",
    "    }\n",
    "\n",
    "    # Find all files and group by type\n",
    "    files_by_type = {key: [] for key in patterns.keys()}\n",
    "    unmatched_files = []\n",
    "\n",
    "    for file_path in source_path.iterdir():\n",
    "        if file_path.is_file():\n",
    "            filename = file_path.name\n",
    "            matched = False\n",
    "\n",
    "            for file_type, pattern in patterns.items():\n",
    "                if re.match(pattern, filename):\n",
    "                    files_by_type[file_type].append(file_path)\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "            if not matched:\n",
    "                unmatched_files.append(file_path)\n",
    "\n",
    "    print(\"📋 Files found:\")\n",
    "    for file_type, files in files_by_type.items():\n",
    "        print(f\"   {file_type}: {len(files)} files\")\n",
    "    if unmatched_files:\n",
    "        print(f\"   unmatched: {len(unmatched_files)} files\")\n",
    "\n",
    "    print(\"\\n📁 Target directories:\")\n",
    "    for file_type, target_dir in target_dirs.items():\n",
    "        print(f\"   {file_type} → {target_dir.name}/\")\n",
    "\n",
    "    if not dry_run:\n",
    "        print(f\"\\n📂 Creating directories...\")\n",
    "        for file_type, target_dir in target_dirs.items():\n",
    "            target_dir.mkdir(exist_ok=True)\n",
    "            print(f\"✅ Created: {target_dir}\")\n",
    "\n",
    "    print(f\"\\n🔄 File operations:\")\n",
    "    print(\"─\" * 60)\n",
    "\n",
    "    total_moves = 0\n",
    "\n",
    "    for file_type, files in files_by_type.items():\n",
    "        if not files:\n",
    "            continue\n",
    "\n",
    "        target_dir = target_dirs[file_type]\n",
    "        print(f\"\\n📂 {file_type.upper()} files → {target_dir.name}/\")\n",
    "\n",
    "        for file_path in files:\n",
    "            target_path = target_dir / file_path.name\n",
    "            print(f\"   {file_path.name} → {target_dir.name}/{file_path.name}\")\n",
    "            total_moves += 1\n",
    "\n",
    "            if not dry_run:\n",
    "                try:\n",
    "                    if target_path.exists():\n",
    "                        print(f\"   ⚠️  Warning: {target_path} already exists, skipping\")\n",
    "                        continue\n",
    "\n",
    "                    shutil.move(str(file_path), str(target_path))\n",
    "                    print(f\"   ✅ Moved successfully\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Error: {e}\")\n",
    "\n",
    "    if unmatched_files:\n",
    "        print(f\"\\n❓ UNMATCHED files (will stay in original directory):\")\n",
    "        for file_path in unmatched_files:\n",
    "            print(f\"   {file_path.name}\")\n",
    "\n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"   Total files to move: {total_moves}\")\n",
    "    print(f\"   Unmatched files: {len(unmatched_files)}\")\n",
    "\n",
    "    if dry_run:\n",
    "        print(f\"\\n💡 This was a dry run. To actually move files, set dry_run=False\")\n",
    "    else:\n",
    "        print(f\"\\n🎉 File splitting completed!\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set your directory path here\n",
    "    source_directory = \"ribo\"  # Change this to your actual path\n",
    "\n",
    "    print(\"🚀 File Splitter Tool\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # First, do a dry run to see what would happen\n",
    "    # print(\"1️⃣  DRY RUN - Showing what would be moved:\")\n",
    "    # split_files_by_type(source_directory, dry_run=False)\n",
    "\n",
    "    # Uncomment the line below to actually perform the splitting\n",
    "    # WARNING: This will actually move your files!\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"2️⃣  ACTUAL SPLITTING - This will move your files:\")\n",
    "    split_files_by_type(source_directory, dry_run=False)"
   ],
   "id": "42fd2e3db4564f26"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
