{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T02:22:31.112391Z",
     "start_time": "2025-07-08T02:22:19.536307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_scheduler\n",
    "from torch.optim import AdamW\n",
    "import pandas as pdf\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nltk.download('punkt')"
   ],
   "id": "dd1fa35749962c7e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamdouh/anaconda3/envs/test/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "[nltk_data] Downloading package punkt to /home/mamdouh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T21:12:00.967432Z",
     "start_time": "2025-07-07T21:12:00.846644Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 3,
   "source": [
    "def load_data(en_file, tr_file):\n",
    "    en_file = \"data_directories/final_data/\" + en_file\n",
    "    tr_file = \"data_directories/final_data/\" + tr_file\n",
    "    # en_file = \"archive/\" + en_file\n",
    "    # tr_file = \"archive/\" + tr_file\n",
    "    with open(en_file, 'r', encoding='utf-8') as file:\n",
    "        en_texts = file.read().strip().split('\\n')\n",
    "    with open(tr_file, 'r', encoding='utf-8') as file:\n",
    "        tr_texts = file.read().strip().split('\\n')\n",
    "    return pd.DataFrame({'en': en_texts, 'tr': tr_texts})\n",
    "\n",
    "# train_data = load_data('new_combined_data_english.txt', \"new_combined_data_akkadian.txt\")\n",
    "train_data = load_data(\"english_train.txt\", \"akkadian_train.txt\")\n"
   ],
   "id": "71fb4916510f1a4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train paraphraser",
   "id": "3da4c60712abe566"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-31T12:49:25.062985Z",
     "start_time": "2025-08-31T12:47:55.929910Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Download PAWS English\n",
    "df_orig = pd.read_csv(\"/mnt/c/Users/user/Downloads/parabank-2.0/parabank2.tsv\",sep='\\t', header=None, names=['score','input', 'target']\n",
    "                ,usecols=[0,1,2])\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamdouh/anaconda3/envs/test/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T12:50:24.672304Z",
     "start_time": "2025-08-31T12:50:23.895747Z"
    }
   },
   "cell_type": "code",
   "source": "df = df_orig.copy()",
   "id": "83639896c85c5f8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T12:50:25.122155Z",
     "start_time": "2025-08-31T12:50:24.674529Z"
    }
   },
   "cell_type": "code",
   "source": "df = df[df['score'] >= 0.45].dropna()[['input', 'target']]\n",
   "id": "7cace5d30d6cecc5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T14:26:48.786823Z",
     "start_time": "2025-07-07T14:26:43.492435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-ROMANCE-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_checkpoint)\n",
    "model = MarianMTModel.from_pretrained(model_checkpoint)\n"
   ],
   "id": "87dc0bdbeb50cd4c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T14:28:36.664303Z",
     "start_time": "2025-07-07T14:26:48.802447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def tokenize_paraphrase_batch(batch, tokenizer=tokenizer, max_length=200):\n",
    "    model_inputs = tokenizer(batch['input'], max_length=max_length, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(batch['target'], max_length=max_length, truncation=True, padding=\"max_length\")['input_ids']\n",
    "    model_inputs['labels'] = labels\n",
    "    return model_inputs\n",
    "\n",
    "# Use debug_df or full_df as needed\n",
    "hf_dataset = Dataset.from_pandas(df)  # or Dataset.from_pandas(full_df)\n",
    "\n",
    "tokenized_dataset = hf_dataset.map(lambda batch: tokenize_paraphrase_batch(batch, tokenizer), batched=True)\n"
   ],
   "id": "cc3cef9c3ea31fa3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/625408 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18e3914ede62428c9dbccccf6193c96f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T14:28:36.772026Z",
     "start_time": "2025-07-07T14:28:36.695118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]"
   ],
   "id": "381e5038539378b7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T14:28:36.785292Z",
     "start_time": "2025-07-07T14:28:36.782907Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 8,
   "source": [
    "def save_model(model, tokenizer, optimizer, dataset_id):\n",
    "    import os\n",
    "    import json\n",
    "    import torch\n",
    "\n",
    "    save_path = \"models/\" + dataset_id\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Save model and tokenizer\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    # Save optimizer state only\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_path, \"optimizer.pt\"))\n"
   ],
   "id": "3eaf4db54e717058"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T14:29:52.366007Z",
     "start_time": "2025-07-07T14:29:52.314569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./paraphraser\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=10000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=5,  # Use 2 for debug, increase as needed\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,  # Set to True if using a GPU that supports it\n",
    "    logging_steps=10000,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "class SaveEveryNEpochsCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, n_epochs=2):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.tok = tokenizer\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        model, optim = kwargs[\"model\"], kwargs[\"optimizer\"]\n",
    "        if int(state.epoch) % self.n_epochs == 0:\n",
    "            save_model(model, self.tok, optim, \"paraphraser\")\n",
    "        return control"
   ],
   "id": "eff07db75d374dd7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T14:29:54.746371Z",
     "start_time": "2025-07-07T14:29:54.733903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[SaveEveryNEpochsCallback(tokenizer, n_epochs=2)]\n",
    ")\n"
   ],
   "id": "360343b82300ab71",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23393/2038214009.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T19:41:52.009464Z",
     "start_time": "2025-07-07T14:29:56.211373Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "a74c4caa28176ab0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='185670' max='185670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [185670/185670 5:11:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.169600</td>\n",
       "      <td>0.125112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>0.110452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.117200</td>\n",
       "      <td>0.103811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.099544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.097077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>0.094837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.099500</td>\n",
       "      <td>0.092848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.092061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.093000</td>\n",
       "      <td>0.090958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.092200</td>\n",
       "      <td>0.089990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>0.091500</td>\n",
       "      <td>0.089130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>0.087800</td>\n",
       "      <td>0.088769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>0.087100</td>\n",
       "      <td>0.088146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.087602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>0.086600</td>\n",
       "      <td>0.087312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>0.084200</td>\n",
       "      <td>0.087026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>0.084100</td>\n",
       "      <td>0.086793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.086559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamdouh/anaconda3/envs/test/lib/python3.10/site-packages/transformers/modeling_utils.py:3339: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=185670, training_loss=0.0995174711828067, metrics={'train_runtime': 18715.6379, 'train_samples_per_second': 158.727, 'train_steps_per_second': 9.921, 'total_flos': 1.57345881587712e+17, 'train_loss': 0.0995174711828067, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:29:51.031949Z",
     "start_time": "2025-07-07T20:29:51.029182Z"
    }
   },
   "cell_type": "code",
   "source": "paraphrase_model = trainer.model",
   "id": "a9f86163f2a6877e",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:26:33.066081Z",
     "start_time": "2025-07-07T20:26:31.038472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "def paraphrase_sentences(sentences, model, tokenizer, max_length=200):\n",
    "    model.eval()\n",
    "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    enc = tokenizer(sentences, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**enc, num_beams=5, max_length=max_length)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "print(paraphrase_sentences(['The NBA season of 1975 -- 76 was the 30th season of the National Basketball Association .'\n",
    "], model, tokenizer))\n"
   ],
   "id": "4af3004dfecb1cfb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"1975's NBA season -- 76 was the 30th season of the National Basketball Association .\"]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T20:26:33.675607Z",
     "start_time": "2025-07-07T20:26:33.101557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for s in [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"He went to the market.\",\n",
    "    \"This is an interesting book.\",\n",
    "    \"I am very happy today.\",\n",
    "    \"The weather is great!\"\n",
    "]:\n",
    "    print(paraphrase_sentences([s], model, tokenizer))\n"
   ],
   "id": "b773ef0861b2a119",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fast brown fox jumps over a lazy dog.']\n",
      "['He went on the market.']\n",
      "[\"That's an intriguing book.\"]\n",
      "[\"Today I'm very happy.\"]\n",
      "[\"Weather's great!\"]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Augment data",
   "id": "a1554038db536ec4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T22:57:02.490649Z",
     "start_time": "2025-07-07T21:20:20.146733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p_paraphrase       = 0.5      # 50 % chance we call the paraphraser for a row\n",
    "k_paraphrases      = 2        # up to 2 alternatives per sentence\n",
    "DEVICE = 'cuda'\n",
    "PARA_MODEL = 'models/paraphraser'\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  1 · Load the paraphraser\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch, pandas as pd, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tok_para = AutoTokenizer.from_pretrained(PARA_MODEL)\n",
    "mdl_para = AutoModelForSeq2SeqLM.from_pretrained(PARA_MODEL).to(DEVICE).eval()\n",
    "\n",
    "def paraphrase(text, num_return=k_paraphrases):\n",
    "    \"\"\"Return ≤k unique paraphrases for *text* (may be fewer if duplicates).\"\"\"\n",
    "    enc    = tok_para(text, return_tensors=\"pt\", truncation=True,\n",
    "                      max_length=200).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outs = mdl_para.generate(\n",
    "            **enc,\n",
    "            num_beams=10,\n",
    "            num_return_sequences=num_return,\n",
    "            temperature=1.0,\n",
    "            diversity_penalty=0.0,\n",
    "            no_repeat_ngram_size=3,\n",
    "            max_length=200,\n",
    "        )\n",
    "    paras = {tok_para.decode(o, skip_special_tokens=True) for o in outs}\n",
    "    paras.discard(text)                    # don’t keep identical copy\n",
    "    return list(paras)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "#  2 · Augment the training DataFrame\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "aug_rows = []           # list of dicts → pd.DataFrame later\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "for _, row in tqdm(train_data.iterrows(), total=len(train_data),\n",
    "                   desc=\"Paraphrasing\"):\n",
    "    tr_sent = row[\"tr\"]\n",
    "    en_sent = row[\"en\"]\n",
    "\n",
    "    # always include the original pair\n",
    "    aug_rows.append({\"tr\": tr_sent, \"en\": en_sent})\n",
    "\n",
    "    # flip a biased coin\n",
    "    if rng.random() < p_paraphrase:\n",
    "        paras = paraphrase(en_sent, num_return=k_paraphrases)\n",
    "        for p in paras:\n",
    "            aug_rows.append({\"tr\": tr_sent, \"en\": p})\n",
    "\n",
    "# build the new DataFrame\n",
    "train_aug = pd.DataFrame(aug_rows).reset_index(drop=True)\n",
    "print(\"New train size:\", len(train_aug))"
   ],
   "id": "b3419674c8b24401",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Paraphrasing:   0%|          | 0/38040 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b86b2b143044615824f16c365ba22d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New train size: 74449\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T02:14:28.666356Z",
     "start_time": "2025-07-08T02:14:27.845201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_aug['en'].to_csv(\"data_directories/final_data/new_combined_paraphrased_english.txt\", index=False)\n",
    "train_aug['tr'].to_csv(\"data_directories/final_data/new_combined_paraphrased_akkadian.txt\", index=False)"
   ],
   "id": "420aa9c2c116bcc6",
   "outputs": [],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
